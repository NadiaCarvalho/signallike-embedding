{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pretty_midi -q\n!pip install torch>=2.0.0 -q\n!pip install tensorboardX -q\n!pip install lightning -q\n\nimport torch\nprint(torch.__version__)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-06-05T16:07:50.743458Z","iopub.execute_input":"2023-06-05T16:07:50.743945Z","iopub.status.idle":"2023-06-05T16:08:43.478047Z","shell.execute_reply.started":"2023-06-05T16:07:50.743902Z","shell.execute_reply":"2023-06-05T16:08:43.476612Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m2.0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Tue Aug 25 16:28:14 2020\n\n@author: prang\n@edited: carvalho\n\"\"\"\n\nimport bz2\nimport os\nfrom abc import ABC, abstractmethod\nfrom bisect import bisect_left\nfrom operator import attrgetter\n\nimport _pickle as cPickle\nimport librosa\nimport numpy as np\nimport pretty_midi\nimport torch  # type: ignore\n\n# %%\n\n# MIDI extensions\nEXT = ['.mid', '.midi', '.MID', '.MIDI']\n# Primes number\nPRIMES = [43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107,\n          109, 113, 127, 131, 137, 149, 157, 163, 167, 173, 179, 191, 197,\n          211, 223, 227, 233, 239, 251, 257, 263, 269, 277, 281, 293, 307,\n          311, 317, 331, 337, 347, 353, 359, 367, 373, 379, 383, 389, 397,\n          401, 409, 419, 431, 439, 443, 449, 457, 461, 467, 479, 487, 491,\n          499, 503, 509, 521, 541, 547, 557, 563, 569, 577, 587, 593, 599,\n          607, 613, 617, 631, 641, 647, 653, 659, 673, 677, 683, 691, 701,\n          709, 719, 727, 733, 739, 743, 751, 757, 761, 769, 773, 787, 797,\n          809, 821, 827, 839, 853, 857, 863, 877, 881, 887, 907, 911, 919,\n          929, 937, 941, 947, 953, 967, 971, 977, 983, 991, 997, 1009, 1013,\n          1019, 1031, 1039, 1049, 1061, 1069, 1087, 1091, 1097, 1103, 1109,\n          1117, 1123, 1129, 1151, 1163, 1171, 1181, 1187, 1193, 1201, 1213,\n          1217, 1223, 1229, 1237, 1249, 1259, 1277, 1283, 1289, 1297, 1301,\n          1307, 1319, 1327, 1361, 1367, 1373, 1381, 1399, 1409, 1423, 1427,\n          1433, 1439, 1447, 1451, 1459, 1471, 1481, 1487, 1493, 1499, 1511,\n          1523, 1531, 1543, 1549, 1553, 1559, 1567, 1571, 1579, 1583, 1597,\n          1601, 1607, 1613, 1619, 1627, 1637, 1657, 1663, 1667, 1693, 1697,\n          1709, 1721, 1733, 1741, 1747, 1753, 1759, 1777, 1783, 1787, 1801,\n          1811, 1823, 1831, 1847, 1861, 1867, 1871, 1877, 1889, 1901, 1907,\n          1913, 1931, 1949, 1973, 1979, 1987, 1993, 1997, 2003, 2011, 2017,\n          2027, 2039, 2053, 2063]\n\n\n# Usefull functions\ndef takeClosest(myList, myNumber):\n    \"\"\"\n    Assumes myList is sorted. Returns closest value to myNumber.\n\n    If two numbers are equally close, return the smallest number.\n    \"\"\"\n    pos = bisect_left(myList, myNumber)\n    if pos == 0:\n        return myList[0]\n    if pos == len(myList):\n        return myList[-1]\n    before = myList[pos - 1]\n    after = myList[pos]\n    if after - myNumber < myNumber - before:\n        return after\n    else:\n        return before\n\n\n# Abstract class for the different input representations\nclass Representation(ABC):\n\n    nb_bars = 0\n    prbar_path = \"\"\n    barfiles = []\n\n    def __init__(self, root_dir, nbframe_per_bar=16, mono=False, export=False):\n        \"\"\"\n        Args:\n            root_dir (string) : Path of the directory with all the MIDI files\n            nbframe_per_bar (int) : Number of frame contained in a bar\n            export (bool) : Force the bar to be exported in .pt file or not\n        \"\"\"\n        assert any((fname.endswith(tuple(EXT))) for fname in os.listdir(\n            root_dir)), \"There are no MIDI files in %s\" % root_dir\n        # root directory path which contains the files\n        self.rootdir = root_dir\n        # midi files names\n        self.midfiles = [fname for fname in os.listdir(\n            root_dir) if (fname.endswith(tuple(EXT)))]\n        # Number of frame per bar\n        self.nbframe_per_bar = nbframe_per_bar\n        # Force export or not\n        self.export = export\n        # Monophonic data (separate voices)\n        self.mono = mono\n        # number of tracks contained in the dataset\n        self.nb_tracks = len(self.midfiles)\n\n    def __len__(self):\n        \"\"\"\n        Return the total number of bars\n        \"\"\"\n        return self.nb_bars  # type: ignore\n\n    def __getitem__(self, index):\n        \"\"\"\n        Return the tensor representation of the bar at the given index\n\n        NOTE: LOADING THE TENSOR FROM THE .PT FILE TAKES A LONGER TIME AND SPACE THAN BZ2\n        \"\"\"\n        # return torch.load(f\"{self.prbar_path}/{self.barfiles[index]}\")\n        with bz2.BZ2File(f\"{self.prbar_path}/{self.barfiles[index]}\", \"rb\") as filepath:\n            return cPickle.load(filepath)\n\n    @abstractmethod\n    def per_bar_export(self):\n        \"\"\"\n        This function take all the midi files, load them into a pretty_midi object.\n        For a complete documentation of pretty_midi go to :\n            http://craffel.github.io/pretty-midi/\n\n        The midi file is then processed to obtain the given representation of each bar with\n        Finally, it will export each of theses bars in a separate .pt\n        \"\"\"\n        pass\n\n\n################################# PIANO-ROLL #################################\n\nclass Pianoroll(Representation):\n\n    def __init__(self, root_dir, nbframe_per_bar=16, mono=False, export=False):\n        super().__init__(root_dir, nbframe_per_bar=nbframe_per_bar, mono=mono, export=export)\n        # Path witch contains the sliced piano-roll\n        if mono:\n            self.prbar_path = root_dir + \\\n                \"/pianoroll_bar_mono_\" + str(self.nbframe_per_bar)\n        else:\n            self.prbar_path = root_dir + \\\n                \"/pianoroll_bar_\" + str(self.nbframe_per_bar)\n\n        if not os.path.exists(self.prbar_path):\n            try:\n                os.mkdir(self.prbar_path)\n            except OSError:\n                print(\"Creation of the directory %s failed\" % self.prbar_path)\n            else:\n                print(\"Successfully created the directory %s \" %\n                      self.prbar_path)\n            # Export the piano-roll bat\n            self.per_bar_export()\n        else:\n            if export:\n                self.per_bar_export()\n        # .pt files names\n        self.barfiles = [fname for fname in os.listdir(\n            self.prbar_path) if fname.endswith('.pbz2')]  # or fname.endswith('.pt')]\n        # total number of bars\n        self.nb_bars = len(self.barfiles)\n\n    def sliced_and_save_pianoroll(self, pianoroll, downbeats, fs, num_bar):\n\n        for i in range(len(downbeats)-1):\n            sp = pianoroll[:, int(round(downbeats[i]*fs))                           :int(round(downbeats[i+1]*fs))-1]\n            if sp.shape[1] > 256:\n                sp = sp[:, 0:256]\n            elif sp.shape[1] < 256 and sp.shape[1] > 0:\n                sp = np.pad(sp, ((0, 0), (0, 256 - sp.shape[1])), 'edge')\n            if sp.shape[1] > 0:\n                # downsample\n                sp = sp[:, ::int(256/self.nbframe_per_bar)]\n                # convert to tensor\n                sp = torch.Tensor(sp)\n                assert (\n                    sp.shape[1] == self.nbframe_per_bar), \"Error, a piano-roll have the wrong size : %s\" % sp.shape[1]\n                # binarize\n                sp[sp != 0] = 1\n\n                # Save the tensor\n                # torch.save(sp.permute(1, 0), f\"{self.prbar_path}/prbar{str(num_bar)}.pt\")\n                with bz2.BZ2File(f\"{self.prbar_path}/prbar{str(num_bar)}.pbz2\", \"w\") as filepath:\n                    cPickle.dump(sp.permute(1, 0), filepath)\n\n                num_bar += 1\n\n        return num_bar\n\n    def per_bar_export(self):\n\n        num_error = 0\n        num_bar = 0\n        # load each .mid file in a pretty_midi object\n        for index in range(len(self.midfiles)):\n            try:\n                midi_data = pretty_midi.PrettyMIDI(\n                    self.rootdir + '/' + self.midfiles[index])\n                downbeats = midi_data.get_downbeats()\n                fs = 257 / (midi_data.get_end_time() / len(downbeats))\n                # If monophonic data is required, we separate each voice\n                if self.mono:\n                    for inst in midi_data.instruments:\n                        if not inst.is_drum:\n                            pianoroll = inst.get_piano_roll(fs=fs)\n                            num_bar = self.sliced_and_save_pianoroll(\n                                pianoroll, downbeats, fs, num_bar)\n                else:\n                    pianoroll = midi_data.get_piano_roll(fs=fs)  # type: ignore\n                    num_bar = self.sliced_and_save_pianoroll(\n                        pianoroll, downbeats, fs, num_bar)\n            except KeyError:\n                num_error += 1\n        print(\"total number of file : \", len(self.midfiles))\n        print('num error : ', num_error)\n\n\n################################# MIDI-like ##################################\n\nclass Midilike(Representation):\n\n    def __init__(self, root_dir, nbframe_per_bar=16, mono=False, export=False):\n        super().__init__(root_dir, nbframe_per_bar=nbframe_per_bar, mono=mono, export=export)\n        # One hot encoding of the vocabulary\n        self.vocabulary = self.get_vocab_encoding()\n        # Path witch contains the sliced piano-roll\n        self.prbar_path = root_dir + \"/MIDIlike_bar\"\n        if not os.path.exists(self.prbar_path):\n            try:\n                os.mkdir(self.prbar_path)\n            except OSError:\n                print(\"Creation of the directory %s failed\" % self.prbar_path)\n            else:\n                print(\"Successfully created the directory %s \" %\n                      self.prbar_path)\n            # Export the piano-roll bat\n            self.per_bar_export()\n        else:\n            if export:\n                self.per_bar_export()\n        # .pt files names\n        self.barfiles = [fname for fname in os.listdir(\n            self.prbar_path) if fname.endswith('.pbz2')]\n        # total number of bars\n        self.nb_bars = len(self.barfiles)\n\n    def get_vocab_encoding(self):\n        \"\"\"\n        Return a dictionnary with the corresponding indexes of every word contained in\n        the vocabulary (one hot encoding).\n\n        e.g : vocab = {'NOTE_ON<1>' : 0\n                       'NOTE_ON<2>' : 1\n                           ...\n                       'TIME_SHIFT<1> : 128\n                           ...             }\n        \"\"\"\n        vocab = {}\n        current_ind = 0\n\n        # All the NOTE_ON events\n        rootstr = \"NOTE_ON<\"\n        for i in range(0, 128):\n            event = rootstr + str(i) + '>'\n            vocab[event] = current_ind\n            current_ind += 1\n        # All the NOTE_OFF events\n        rootstr = \"NOTE_OFF<\"\n        for i in range(0, 128):\n            event = rootstr + str(i) + '>'\n            vocab[event] = current_ind\n            current_ind += 1\n        # All the TIME_SHIFT events\n        rootstr = \"TIME_SHIFT<\"\n        for i in range(10, 1001, 10):\n            event = rootstr + str(i) + '>'\n            vocab[event] = current_ind\n            current_ind += 1\n        # All the SET_VELOCITY events\n        rootstr = \"SET_VELOCITY<\"\n        for i in range(0, 128, 4):\n            event = rootstr + str(i) + '>'\n            vocab[event] = current_ind\n            current_ind += 1\n        # The NOTHING event\n        vocab['NOTHING'] = current_ind\n\n        return vocab\n\n    def string_representation(self, Vinst):\n        \"\"\"\n        Return the representation with string (\"NOTE_ON<56>, ...) from the\n        corresponding integer representation Vinst (list of int)\n        \"\"\"\n        str_rep = []\n        for i in Vinst:\n            str_rep.append(list(self.vocabulary.keys())[\n                           list(self.vocabulary.values()).index(int(i))])\n\n        return str_rep\n\n    def per_bar_export(self):\n        \"\"\"\n        This function take all the midi files, load them into a pretty_midi object.\n        For a complete documentation of pretty_midi go to :\n            http://craffel.github.io/pretty-midi/\n\n        The midi file is then processed to obtain a MIDI-like event-based representation.\n        More info on this representation here :\n            https://arxiv.org/pdf/1809.04281.pdf\n\n        Ex :  SET_VELOCITY<80>, NOTE_ON<60>\n              TIME_SHIFT<500>, NOTE_ON<64>\n              TIME_SHIFT<500>, NOTE_ON<67>\n              TIME_SHIFT<1000>, NOTE_OFF<60>, NOTE_OFF<64>, NOTE_OFF<67>\n              TIME_SHIFT<500>, SET_VELOCITY<100>, NOTE_ON<65>\n              TIME_SHIFT<500>, NOTE_OFF<65>\n\n        Finally, it will export each of theses bars in a separate .pt\n        \"\"\"\n        # number of error with the key analyzer\n        num_error = 0\n        # array of all the tensor representing each bar\n        all_bars = []\n        # load each .mid file in a pretty_midi object\n        for index in range(len(self.midfiles)):\n            try:\n                midi_data = pretty_midi.PrettyMIDI(\n                    self.rootdir + '/' + self.midfiles[index])\n                # start_time = midi_data.estimate_beat_start()\n                downbeats = midi_data.get_downbeats()\n                current_velocity = 64.\n                # Possible value for the velocity\n                velocity_list = [i for i in range(0, 128, 4)]\n                # Possible value for the time shifts\n                timeshift_list = [i for i in range(10, 1001, 10)]\n                for i in range(len(downbeats)-1):\n                    list_notes = []\n                    V = []\n                    for inst in midi_data.instruments:\n                        if not inst.is_drum:\n                            for n in inst.notes:\n                                if (n.start < downbeats[i+1] and n.end >= downbeats[i]):\n                                    list_notes.append(n)\n                    # Sort list by pitch\n                    list_notes = sorted(\n                        list_notes, key=lambda i: i.pitch, reverse=False)\n                    if len(list_notes) == 0:\n                        gap = (downbeats[i+1] - downbeats[i])*1000\n                        while (gap > 1000):\n                            V.append(self.vocabulary['TIME_SHIFT<1000>'])\n                            gap = gap - 1000\n                        timeshift = takeClosest(timeshift_list, gap)\n                        V.append(\n                            self.vocabulary['TIME_SHIFT<' + str(timeshift) + '>'])\n                    else:\n                        # iterate over list_notes to construct the representation\n                        current_time = downbeats[i]\n                        while (list_notes):\n                            closest_note_on = min(\n                                list_notes, key=attrgetter('start'))\n                            closest_note_off = min(\n                                list_notes, key=attrgetter('end'))\n                            if closest_note_off.end > closest_note_on.start:\n                                gap = (closest_note_on.start -\n                                       current_time)*1000\n                                if gap > timeshift_list[0]/2:\n                                    while (gap > 1000):\n                                        V.append(\n                                            self.vocabulary['TIME_SHIFT<1000>'])\n                                        gap = gap - 1000\n                                    timeshift = takeClosest(\n                                        timeshift_list, gap)\n                                    V.append(\n                                        self.vocabulary['TIME_SHIFT<' + str(timeshift) + '>'])\n                                if takeClosest(velocity_list, closest_note_on.velocity) != current_velocity:\n                                    veloc = takeClosest(\n                                        velocity_list, closest_note_on.velocity)\n                                    V.append(\n                                        self.vocabulary['SET_VELOCITY<' + str(veloc) + '>'])\n                                    current_velocity = veloc\n                                V.append(\n                                    self.vocabulary['NOTE_ON<' + str(closest_note_on.pitch) + '>'])\n                                if closest_note_on.start > current_time:\n                                    current_time = closest_note_on.start\n                                if closest_note_on.end > downbeats[i+1]:\n                                    list_notes.remove(closest_note_on)\n                                else:\n                                    # Set a value > end to start to not taking it in account anymore\n                                    closest_note_on.start = closest_note_on.end + 10\n                            else:\n                                gap = (closest_note_off.end -\n                                       current_time)*1000\n                                if gap > timeshift_list[0]/2:\n                                    while (gap > 1000):\n                                        V.append(\n                                            self.vocabulary['TIME_SHIFT<1000>'])\n                                        gap = gap - 1000\n                                    timeshift = takeClosest(\n                                        timeshift_list, gap)\n                                    V.append(\n                                        self.vocabulary['TIME_SHIFT<' + str(timeshift) + '>'])\n                                V.append(\n                                    self.vocabulary['NOTE_OFF<' + str(closest_note_off.pitch) + '>'])\n                                current_time = closest_note_off.end\n                                list_notes.remove(closest_note_off)\n                    # Store the tensor in all_bars\n                    all_bars.append(torch.tensor(V))\n            except KeyError:\n                num_error += 1\n        print('num error : ', num_error)\n        # Cleaning of the tensor : supressing ones with more than 160 events\n        # and padding to have a constant size equal to 160\n        empty_bar = False\n        total_num = 0\n        for i, vec in enumerate(all_bars):\n            # add the empty bar only one time\n            if len(vec) == 1:\n                if not empty_bar:\n                    clean_vec = torch.tensor([self.vocabulary['NOTHING']]*64)\n                    clean_vec[0] = vec\n                    # torch.save(clean_vec.unsqueeze(\n                    #     1), self.prbar_path + \"/Mlikebar_\" + str(i) + \".pt\")\n                    with bz2.BZ2File(f\"{self.prbar_path}/Mlikebar_{str(i)}.pbz2\", \"w\") as filepath:\n                        cPickle.dump(clean_vec.unsqueeze(1), filepath)\n                    empty_bar = True\n                    total_num += 1\n            elif len(vec) < 64:\n                clean_vec = torch.tensor([self.vocabulary['NOTHING']]*64)\n                clean_vec[:len(vec)] = vec\n                # torch.save(clean_vec.unsqueeze(1), self.prbar_path +\n                #            \"/Mlikebar_\" + str(i) + \".pt\")\n                with bz2.BZ2File(f\"{self.prbar_path}/Mlikebar_{str(i)}.pbz2\", \"w\") as filepath:\n                    cPickle.dump(clean_vec.unsqueeze(1), filepath)\n                total_num += 1\n            elif len(vec) == 64:\n                # torch.save(vec.unsqueeze(1), self.prbar_path +\n                #            \"/Mlikebar_\" + str(i) + \".pt\")\n                with bz2.BZ2File(f\"{self.prbar_path}/Mlikebar_{str(i)}.pbz2\", \"w\") as filepath:\n                    cPickle.dump(vec.unsqueeze(1), filepath)\n                total_num += 1\n        print(\"Initial number of bar : {}\\n \\\n               After cleaning : {}\\n \\\n               Number of suppression : {}\".format(len(all_bars), total_num, len(all_bars) - total_num))\n\n\n############################### MIDI-like mono ###############################\n\nclass Midimono(Representation):\n\n    def __init__(self, root_dir, nbframe_per_bar=16, mono=True, export=False):\n        super().__init__(root_dir, nbframe_per_bar=nbframe_per_bar, mono=mono, export=export)\n        # Path witch contains the sliced piano-roll\n        self.prbar_path = root_dir + \"/MIDIMono_bar\"\n        if not os.path.exists(self.prbar_path):\n            try:\n                os.mkdir(self.prbar_path)\n            except OSError:\n                print(\"Creation of the directory %s failed\" % self.prbar_path)\n            else:\n                print(\"Successfully created the directory %s \" %\n                      self.prbar_path)\n            # Export the piano-roll bat\n            self.per_bar_export()\n        else:\n            if export:\n                self.per_bar_export()\n        # .pt files names\n        self.barfiles = [fname for fname in os.listdir(\n            self.prbar_path) if fname.endswith('.pbz2')]\n        # total number of bars\n        self.nb_bars = len(self.barfiles)\n\n    def get_polyphonic_bars(self, pr_dataset):\n\n        indices = set()\n        for i in range(len(pr_dataset)):\n            for j, frame in enumerate(pr_dataset[i]):\n                if frame.nonzero().nelement() > 1:\n                    indices.add(i)\n\n        return indices\n\n    def to_pianoroll(self, v):\n\n        pianoroll = torch.zeros(16, 128)\n        current_note = -1\n        for i, e in enumerate(v):\n            if e < 128:\n                pianoroll[i, int(e)] = 1\n                current_note = int(e)\n            elif e == 128:\n                if current_note != 129:\n                    pianoroll[i, current_note] = 1\n\n        return pianoroll\n\n    def per_bar_export(self):\n\n        PR = Pianoroll(self.rootdir, nbframe_per_bar=16, mono=True)\n        poly_bars = self.get_polyphonic_bars(PR)\n        num_vec = 0\n        for i in range(len(PR)):\n            if i not in poly_bars:\n                vec = torch.zeros(16)\n                current_note = -1\n                for j, frame in enumerate(PR[i]):\n                    if frame.nonzero().nelement() == 0:\n                        if current_note != 129 and current_note != -1:\n                            # note_off event\n                            vec[j] = 129\n                            current_note = 129\n                        else:\n                            # rest event\n                            vec[j] = 128\n                            if current_note == -1:\n                                current_note = 129\n                    else:\n                        if current_note == int(frame.nonzero()):\n                            # rest event\n                            vec[j] = 128\n                        else:\n                            # note_on event\n                            vec[j] = int(frame.nonzero())\n                            current_note = int(frame.nonzero())\n                # Save the tensor\n                # torch.save(vec.unsqueeze(1), self.prbar_path +\n                #            \"/MVAEbar_\" + str(num_vec) + \".pt\")\n                with bz2.BZ2File(f\"{self.prbar_path}/MVAEbar_{str(num_vec)}.pbz2\", \"w\") as filepath:\n                    cPickle.dump(vec.unsqueeze(1), filepath)\n                num_vec += 1\n\n\n################################# NoteTuple ##################################\n\nclass Notetuple(Representation):\n\n    def __init__(self, root_dir, nbframe_per_bar=16, mono=False, export=False):\n        super().__init__(root_dir, nbframe_per_bar=nbframe_per_bar, mono=mono, export=export)\n        # vocabs\n        self.vocabs = self.get_vocabs_encoding()\n        self.ts_major = self.vocabs[0]\n        self.ts_minor = self.vocabs[1]\n        self.dur_major = self.vocabs[2]\n        self.dur_minor = self.vocabs[3]\n        # Path witch contains the sliced piano-roll\n        self.prbar_path = root_dir + \"/NoteTuple_bar\"\n        if not os.path.exists(self.prbar_path):\n            try:\n                os.mkdir(self.prbar_path)\n            except OSError:\n                print(\"Creation of the directory %s failed\" % self.prbar_path)\n            else:\n                print(\"Successfully created the directory %s \" %\n                      self.prbar_path)\n            # Export the bar\n            self.per_bar_export()\n        else:\n            if export:\n                self.per_bar_export()\n        # .pt files names\n        self.barfiles = [fname for fname in os.listdir(\n            self.prbar_path) if fname.endswith('.pbz2')]\n        # total number of bars\n        self.nb_bars = len(self.barfiles)\n\n    def get_vocabs_encoding(self):\n        # timeshift major_ticks_vocab\n        ts_major = {}\n        ind = 0\n        for val in [i for i in range(0, 9601, 800)]:\n            ts_major[val] = ind\n            ind += 1\n        ts_major[-1] = ind\n        # timeshift minor_ticks_vocab\n        ts_minor = {}\n        ind = 0\n        for val in [i for i in range(0, 800, 10)]:\n            ts_minor[val] = ind\n            ind += 1\n        ts_minor[-1] = ind\n        # duration major_ticks_vocab\n        dur_major = {}\n        ind = 0\n        for val in [i for i in range(0, 9501, 500)]:\n            dur_major[val] = ind\n            ind += 1\n        dur_major[-1] = ind\n        # duration minor_ticks_vocab\n        dur_minor = {}\n        ind = 0\n        for val in [i for i in range(0, 500, 10)]:\n            dur_minor[val] = ind\n            ind += 1\n        dur_minor[-1] = ind\n        return ts_major, ts_minor, dur_major, dur_minor\n\n    def value_to_class(self, bar):\n        # Change the value of the timeshift and duration to a class number\n        for i, tupl in enumerate(bar):\n            for j, v in enumerate(tupl):\n                if j == 0:\n                    bar[i][j] = self.ts_major[int(v)]\n                if j == 1:\n                    bar[i][j] = self.ts_minor[int(v)]\n                if j == 2:\n                    if v == -1:\n                        bar[i][j] = 128\n                if j == 3:\n                    bar[i][j] = self.dur_major[int(v)]\n                if j == 4:\n                    bar[i][j] = self.dur_minor[int(v)]\n        return bar\n\n    def class_to_value(self, bar):\n        # Change the class number of the timeshift and duration to the real value\n        for i, tupl in enumerate(bar):\n            for j, v in enumerate(tupl):\n                if j == 0:\n                    bar[i][j] = list(self.ts_major.keys())[\n                        list(self.ts_major.values()).index(int(v))]\n                if j == 1:\n                    bar[i][j] = list(self.ts_minor.keys())[\n                        list(self.ts_minor.values()).index(int(v))]\n                if j == 2:\n                    if v == 128:\n                        bar[i][j] = -1\n                if j == 3:\n                    bar[i][j] = list(self.dur_major.keys())[\n                        list(self.dur_major.values()).index(int(v))]\n                if j == 4:\n                    bar[i][j] = list(self.dur_minor.keys())[\n                        list(self.dur_minor.values()).index(int(v))]\n        return bar\n\n    def per_bar_export(self):\n        num_error = 0\n        # to store all the bars\n        all_bars = []\n        for index in range(len(self.midfiles)):\n            try:\n                # load each .mid file in a pretty_midi object\n                midi_data = pretty_midi.PrettyMIDI(\n                    self.rootdir + '/' + self.midfiles[index])\n                # start_time = midi_data.estimate_beat_start()\n                downbeats = midi_data.get_downbeats()\n                # Possible value for the time shifts (from 0 to 10s)\n                # 13 major ticks\n                timeshift_major_ticks = [i for i in range(0, 9601, 800)]\n                # 77 minor ticks\n                timeshift_minor_ticks = [i for i in range(0, 800, 10)]\n                # Possible value for the duration\n                # 13 major ticks\n                dur_major_ticks = [i for i in range(0, 9501, 500)]\n                # 77 minor ticj=ks\n                dur_minor_ticks = [i for i in range(0, 500, 10)]\n                for i in range(len(downbeats)-1):\n                    list_notes = []\n                    V = []\n                    for inst in midi_data.instruments:\n                        if not inst.is_drum:\n                            for n in inst.notes:\n                                if (n.start < downbeats[i+1] and n.start >= downbeats[i]):\n                                    list_notes.append(n)\n                    list_notes = sorted(\n                        list_notes, key=lambda i: i.pitch, reverse=False)\n                    # iterate over list_notes to construct the representation\n                    current_time = downbeats[i]\n                    while (list_notes):\n                        closest_note_on = min(\n                            list_notes, key=attrgetter('start'))\n                        time_shift = (closest_note_on.start -\n                                      current_time)*1000\n                        tmat = timeshift_major_ticks[int(time_shift//800)]\n                        tmit = timeshift_minor_ticks[int(\n                            (time_shift % 800)//10)]\n                        duration = (closest_note_on.end -\n                                    closest_note_on.start)*1000\n                        dmat = dur_major_ticks[int(duration//500)]\n                        dmit = dur_minor_ticks[int((duration % 500)//10)]\n                        current_time = closest_note_on.start\n                        V.append(\n                            (tmat, tmit, closest_note_on.pitch, dmat, dmit))\n                        list_notes.remove(closest_note_on)\n                    # Store the tensor in all_bars\n                    all_bars.append(torch.tensor(V))\n            except KeyError:\n                num_error += 1\n        print('num error : ', num_error)\n        # Save all tensor\n        total_num = 0\n        for i, vec in enumerate(all_bars):\n            if len(vec) < 32 and len(vec) > 0:\n                clean_vec = torch.zeros(32, 5).fill_(-1)\n                clean_vec[:len(vec)] = vec\n                clean_vec = self.value_to_class(clean_vec)\n                # torch.save((clean_vec, len(vec)), self.prbar_path +\n                #            \"/Ntuplebar\" + str(i) + \".pt\")\n                with bz2.BZ2File(f\"{self.prbar_path}/Ntuplebar{str(i)}.pbz2\", \"w\") as filepath:\n                    cPickle.dump((clean_vec, len(vec)), filepath)\n                total_num += 1\n            elif len(vec) == 32:\n                vec = self.value_to_class(vec)\n                # torch.save((clean_vec, len(vec)), self.prbar_path +\n                #            \"/Ntuplebar\" + str(i) + \".pt\")\n                with bz2.BZ2File(f\"{self.prbar_path}/Ntuplebar{str(i)}.pbz2\", \"w\") as filepath:\n                    cPickle.dump((clean_vec, len(vec)),  # type: ignore\n                                 filepath)\n                total_num += 1\n        print(\"Initial number of bar : {}\\n \\\n               After cleaning : {}\\n \\\n               Number of suppression : {}\".format(len(all_bars), total_num, len(all_bars) - total_num))\n\n\n################################# Signal-like ################################\n\nclass Signallike(Representation):\n\n    def __init__(self, root_dir, nbframe_per_bar=16, mono=False, export=False):\n        super().__init__(root_dir, nbframe_per_bar=nbframe_per_bar, mono=mono, export=export)\n        # Path to export the .pt files\n        if self.mono:\n            self.prbar_path = root_dir + \\\n                \"/Signallike_bar_mono_\" + str(self.nbframe_per_bar)\n        else:\n            self.prbar_path = root_dir + \\\n                \"/Signallike_bar_\" + str(self.nbframe_per_bar)\n        if not os.path.exists(self.prbar_path):\n            try:\n                os.mkdir(self.prbar_path)\n            except OSError:\n                print(\"Creation of the directory %s failed\" % self.prbar_path)\n            else:\n                print(\"Successfully created the directory %s \" %\n                      self.prbar_path)\n            # Export the piano-roll bat\n            self.per_bar_export()\n        else:\n            if export:\n                self.per_bar_export()\n        # .pt files names\n        self.barfiles = [fname for fname in os.listdir(\n            self.prbar_path) if fname.endswith('.pbz2')]\n        # total number of bars\n        self.nb_bars = len(self.barfiles)\n        # Size of the signal representation\n        self.signal_size = len(self.__getitem__(0).flatten())\n\n    def back_to_pianoroll(self, V):\n        \"\"\"\n        Inverse the process : get a piano-roll from a signal-like representation V.\n        \"\"\"\n        PR = ((np.abs(librosa.core.stft(V, n_fft=2048, window='blackman'))))[\n            PRIMES[:128]]\n        return abs(PR)\n\n    def get_polyphonic_bars(self, pr_dataset):\n        indices = set()\n        for i in range(len(pr_dataset)):\n            for j, frame in enumerate(pr_dataset[i]):\n                if frame.nonzero().nelement() > 1:\n                    indices.add(i)\n        return indices\n\n    def per_bar_export(self):\n        \"\"\"\n        This function take the self.midfiles[index], load it into a pretty_midi object.\n        For a complete documentation of pretty_midi go to :\n            http://craffel.github.io/pretty-midi/\n\n        The midi file is then processed with stft to obtain a signal-like representation\n        and exported in a .pt file.\n        \"\"\"\n        PR = Pianoroll(\n            self.rootdir, nbframe_per_bar=self.nbframe_per_bar, mono=self.mono)\n        if self.mono:\n            poly_bars = self.get_polyphonic_bars(PR)\n        else:\n            poly_bars = []\n        for i in range(len(PR)):\n            if i not in poly_bars:\n                final_vals = np.zeros(\n                    (1025, PR[i].permute(1, 0).shape[1])).astype(complex)\n                final_vals[PRIMES[:128], :] = np.array(PR[i].permute(\n                    1, 0)) + 1j * ((np.array(PR[i].permute(1, 0)) > 0))\n                V = torch.Tensor(librosa.core.istft(\n                    final_vals, window='blackman'))\n                # torch.save(V.reshape(64, -1), self.prbar_path +\n                #            \"/Slikebar_\" + str(i) + \".pt\")\n                with bz2.BZ2File(f\"{self.prbar_path}/Slikebar_{str(i)}.pbz2\", \"w\") as filepath:\n                    cPickle.dump(V.reshape(64, -1), filepath)\n\n################################# DFT-128 ################################\n\n\ndef dft_reduction(data, normalize=False, return_complex=False, only_dft=False):\n    \"\"\"GET DFT\"\"\"\n    dft = np.fft.fft(data)\n\n    if normalize:\n        # GET ENERGY\n        energy = dft[0].real\n        # REDUCE AND NORMALIZE DFT\n        reduced_dft = dft[1: int(len(dft) / 2.0) + 1]\n        norm_dft = [df / energy for df in reduced_dft]\n        # GET MAGNITUDE\n        mag = [abs(CP) for CP in norm_dft]\n    else:\n        norm_dft = dft\n        energy = dft[0].real\n        mag = [abs(CP) for CP in norm_dft]\n\n    if return_complex:\n        if only_dft:\n            return norm_dft\n        return norm_dft, energy, mag\n\n    real_dft = []\n    for complex_coefficient in norm_dft:\n        real_dft.append(complex_coefficient.real)\n        real_dft.append(complex_coefficient.imag)\n\n    if only_dft:\n        return real_dft\n    # RETURN\n    return real_dft, energy, mag\n\n\nclass DFT128(Representation):\n    def __init__(self, root_dir, nbframe_per_bar=16, mono=False, export=False, use_symmetry=True):\n        super().__init__(root_dir, nbframe_per_bar=nbframe_per_bar, mono=mono, export=export)\n\n        self.prbar_path = root_dir + \"/DFT128_bar\" + str(self.nbframe_per_bar)\n        self.use_symmetry = use_symmetry\n\n        if not os.path.exists(self.prbar_path):\n            try:\n                os.mkdir(self.prbar_path)\n            except OSError:\n                print(\"Creation of the directory %s failed\" % self.prbar_path)\n            else:\n                print(\"Successfully created the directory %s \" %\n                      self.prbar_path)\n\n            # Export the piano-roll bat\n            self.per_bar_export()\n        elif export:\n            self.per_bar_export()\n\n        # .pt files names\n        self.barfiles = [fname for fname in os.listdir(\n            self.prbar_path) if fname.endswith('.pbz2')]\n        # total number of bars\n        self.nb_bars = len(self.barfiles)\n        # Size of the signal representation\n        self.signal_size = len(self.__getitem__(0).flatten())\n\n    def back_to_pianoroll(self, V):\n        \"\"\"\n        Inverse the process : get a piano-roll from a DFT128 representation V.\n        \"\"\"\n        return None\n\n    def per_bar_export(self):\n        \"\"\"\n        This function take the self.midfiles[index], load it into a pretty_midi object.\n                For a complete documentation of pretty_midi go to :\n            http://craffel.github.io/pretty-midi/\n\n        The midi file is then processed with DFT encodings to obtain a DFT128 representation\n        \"\"\"\n        PR = Pianoroll(\n            self.rootdir, nbframe_per_bar=self.nbframe_per_bar, mono=self.mono)\n        for i in range(len(PR)):\n            dft_results = np.apply_along_axis(\n                dft_reduction, 1, PR[i].numpy(), only_dft=True)\n            if self.use_symmetry:\n                dft_results = torch.Tensor(dft_results[:, :65*2])\n            else:\n                dft_results = torch.Tensor(dft_results)\n            with bz2.BZ2File(f\"{self.prbar_path}/DFT128bar_{str(i)}.pbz2\", \"w\") as filepath:\n                cPickle.dump(dft_results, filepath)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-06-05T16:08:43.481501Z","iopub.execute_input":"2023-06-05T16:08:43.481840Z","iopub.status.idle":"2023-06-05T16:08:43.619149Z","shell.execute_reply.started":"2023-06-05T16:08:43.481811Z","shell.execute_reply":"2023-06-05T16:08:43.617845Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Training Network","metadata":{}},{"cell_type":"code","source":"arguments = {\n    '--path': '/kaggle/input/jsb-chorales-signallike-embeddings/dataset',\n    #'--save_dt': '/kaggle/working/dataset',\n    '--o': '/kaggle/working/output',\n    '--runname': 'DFT128_01',\n    '--save': True,\n\n    '--gpu': True if torch.cuda.is_available() else False,\n    '--mps': True if torch.backends.mps.is_available() else False,\n    '--gpudev': 0,\n\n    '--lr': 1e-4,\n    '--bsize': 16,\n    '--nbframe': 64,\n    '--inputrep': 'dft128',\n    '--epochs': 10,\n}\n\nprint(arguments)","metadata":{"execution":{"iopub.status.busy":"2023-06-05T16:08:43.621478Z","iopub.execute_input":"2023-06-05T16:08:43.621877Z","iopub.status.idle":"2023-06-05T16:08:43.631859Z","shell.execute_reply.started":"2023-06-05T16:08:43.621834Z","shell.execute_reply":"2023-06-05T16:08:43.630798Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"{'--path': '/kaggle/input/jsb-chorales-signallike-embeddings/dataset', '--o': '/kaggle/working/output', '--runname': 'DFT128_01', '--save': True, '--gpu': True, '--mps': False, '--gpudev': 0, '--lr': 0.0001, '--bsize': 16, '--nbframe': 64, '--inputrep': 'dft128', '--epochs': 10}\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport time\n\nimport lightning as L\nimport torch\n\nfrom docopt import docopt\nfrom tqdm import tqdm\n\nos.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n\n# Set detect anomaly\ntorch.autograd.set_detect_anomaly(True)  # type: ignore\n\n# Parameters\ntrain_path = arguments['--path'] + '/train'\ntest_path = arguments['--path'] + '/test'\nbatch_size = int(arguments['--bsize'])\nnb_frame = int(arguments['--nbframe'])\nif arguments['--o'] == 'None':\n    output_dr = os.getcwd() + '/output'\nelse:\n    output_dr = arguments['--o']\n\n# load the dataset\nif arguments['--inputrep'] == \"pianoroll\":\n    dataset = Pianoroll(train_path, nbframe_per_bar=nb_frame)\n    testset = Pianoroll(test_path, nbframe_per_bar=nb_frame)\n    input_dim = 128\n    seq_length = nb_frame\nelif arguments['--inputrep'] == \"midilike\":\n    dataset = Midilike(train_path)\n    testset = Midilike(test_path)\n    input_dim = 1\nelif arguments['--inputrep'] == \"midimono\":\n    dataset = Midimono(train_path)\n    testset = Midimono(test_path)\n    input_dim = 1\nelif arguments['--inputrep'] == \"signallike\":\n    dataset = Signallike(\n        train_path, nbframe_per_bar=nb_frame*2, mono=True)\n    testset = Signallike(\n        test_path, nbframe_per_bar=nb_frame*2, mono=True)\n    input_dim = dataset.signal_size//64\nelif arguments['--inputrep'] == \"notetuple\":\n    dataset = Notetuple(train_path)\n    testset = Notetuple(test_path)\n    input_dim = 5\nelif arguments['--inputrep'] == \"dft128\":\n    dataset = DFT128(train_path, nbframe_per_bar=nb_frame)\n    testset = DFT128(test_path, nbframe_per_bar=nb_frame)\n    input_dim = 130\n    seq_length = nb_frame\nelse:\n    raise NotImplementedError(\n        \"Representation {} not implemented\".format(arguments['--inputrep']))\n\n# Init the dataloader\ndata_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=2,  # type: ignore\n                                          pin_memory=True, shuffle=True, drop_last=True)\ntest_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, num_workers=2,  # type: ignore\n                                          pin_memory=True, shuffle=False, drop_last=True)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-06-05T16:08:43.635199Z","iopub.execute_input":"2023-06-05T16:08:43.635617Z","iopub.status.idle":"2023-06-05T16:08:44.737103Z","shell.execute_reply.started":"2023-06-05T16:08:43.635575Z","shell.execute_reply":"2023-06-05T16:08:44.735681Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Model parameters\nenc_hidden_size = 1024\ncond_hidden_size = 1024\ndec_hidden_size = 1024\ncond_outdim = 512\nnum_layers_enc = 2\nnum_layers_dec = 2\nnum_subsequences = 4\nlatent_size = 256\n\nif arguments['--inputrep'] in ['pianoroll', 'signallike', 'dft128']:\n    output_dim = input_dim\nelif arguments['--inputrep'] == \"midilike\":\n    output_dim = len(dataset.vocabulary)  # type: ignore\n    seq_length = 64\nelif arguments['--inputrep'] == \"midimono\":\n    output_dim = 130\n    seq_length = 16\nelif arguments['--inputrep'] == \"notetuple\":\n    output_dim = sum([len(v) for v in\n                      dataset.vocabs]) + 129  # type: ignore\n    seq_length = 32\n\ndevice = 'cpu'\nif arguments['--gpu'] and torch.cuda.is_available():  # type: ignore\n    device = 'cuda'\nelif arguments['--mps'] and torch.backends.mps.is_available() and torch.backends.mps.is_built():  # type: ignore\n    device = 'mps'\n\n# Instanciate model\nencoder = Encoder_RNN(input_dim, enc_hidden_size,\n                        latent_size, num_layers_enc, device=device)\ndecoder = Decoder_RNN_hierarchical(output_dim, latent_size, cond_hidden_size,  # type: ignore\n                                     cond_outdim, dec_hidden_size=dec_hidden_size, num_layers=num_layers_dec,\n                                     num_subsequences=num_subsequences, seq_length=seq_length)  # type: ignore\n\nif arguments['--inputrep'] == \"notetuple\":\n    model = LightningVAE(encoder, decoder, arguments['--inputrep'],\n                           vocab=dataset.vocabs)  # type: ignore\nelse:\n    model = LightningVAE(encoder, decoder, arguments['--inputrep'])","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-06-05T16:08:44.739111Z","iopub.execute_input":"2023-06-05T16:08:44.740011Z","iopub.status.idle":"2023-06-05T16:08:45.405075Z","shell.execute_reply.started":"2023-06-05T16:08:44.739966Z","shell.execute_reply":"2023-06-05T16:08:45.404090Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Tue Aug 25 13:41:24 2020\n\n@author: prang\n\"\"\"\n\nimport random\nfrom typing import Any\n\nimport lightning as L\nimport torch  # type: ignore\nfrom lightning.pytorch.utilities.types import STEP_OUTPUT\nfrom torch import nn  # type: ignore\n\n\nclass Encoder_RNN(nn.Module):\n\n    def __init__(self, input_dim, hidden_size, latent_size, num_layers,\n                 dropout=0.5, packed_seq=False, device='cpu'):\n        \"\"\" This initializes the encoder \"\"\"\n        super(Encoder_RNN, self).__init__()\n\n        # Parameters\n        self.num_layers = num_layers\n        self.hidden_size = hidden_size\n        self.latent_size = latent_size\n        self.packed_seq = packed_seq\n        self.batch_first = True\n        self.device = device\n\n        # Layers\n        self.RNN = nn.LSTM(input_dim, hidden_size, batch_first=self.batch_first,\n                           num_layers=num_layers, bidirectional=True,\n                           dropout=dropout)\n\n    def forward(self, x, h0, c0, batch_size):\n\n        # Pack sequence if needed\n        if self.packed_seq:\n            x = torch.nn.utils.rnn.pack_padded_sequence(x[0], x[1],\n                                                        batch_first=self.batch_first,\n                                                        enforce_sorted=False)\n        # Forward pass\n        _, (h, _) = self.RNN(x, (h0, c0))\n\n        # Be sure to not have NaN values\n        assert ((h == h).all()), 'NaN value in the output of the RNN, try to \\\n                                lower your learning rate'\n        h = h.view(self.num_layers, 2, batch_size, -1)\n        h = h[-1]\n        h = torch.cat([h[0], h[1]], dim=1)\n\n        return h\n\n    def init_hidden(self, batch_size=1):\n        # Bidirectional -> num_layers * 2\n        return (torch.zeros(self.num_layers * 2, batch_size, self.hidden_size,\n                            dtype=torch.float, device=self.device),) * 2\n\n\nclass Decoder_RNN_hierarchical(nn.Module):\n\n    def __init__(self, input_size, latent_size, cond_hidden_size, cond_outdim,\n                 dec_hidden_size, num_layers, num_subsequences, seq_length,\n                 teacher_forcing_ratio=0, dropout=0.5):\n        \"\"\" This initializes the decoder \"\"\"\n        super(Decoder_RNN_hierarchical, self).__init__()\n\n        # Parameters\n        self.num_subsequences = num_subsequences\n        self.input_size = input_size\n        self.num_layers = num_layers\n        self.seq_length = seq_length\n        self.teacher_forcing_ratio = teacher_forcing_ratio\n        self.subseq_size = self.seq_length // self.num_subsequences\n\n        # Layers\n        self.tanh = nn.Tanh()\n        self.fc_init_cond = nn.Linear(\n            latent_size, cond_hidden_size * num_layers)\n        self.conductor_RNN = nn.LSTM(latent_size // num_subsequences, cond_hidden_size,\n                                     batch_first=True, num_layers=num_layers,\n                                     bidirectional=False, dropout=dropout)\n        self.conductor_output = nn.Linear(cond_hidden_size, cond_outdim)\n        self.fc_init_dec = nn.Linear(cond_outdim, dec_hidden_size * num_layers)\n        self.decoder_RNN = nn.LSTM(cond_outdim + input_size, dec_hidden_size,\n                                   batch_first=True, num_layers=num_layers,\n                                   bidirectional=False, dropout=dropout)\n        self.decoder_output = nn.Linear(dec_hidden_size, input_size)\n\n    def forward(self, latent, target, batch_size, teacher_forcing, device):\n\n        # Get the initial state of the conductor\n        h0_cond = self.tanh(self.fc_init_cond(latent))\n        h0_cond = h0_cond.view(self.num_layers, batch_size, -1).contiguous()\n        # Divide the latent code in subsequences\n        latent = latent.view(batch_size, self.num_subsequences, -1)\n        # Pass through the conductor\n        subseq_embeddings, _ = self.conductor_RNN(latent, (h0_cond,)*2)\n        subseq_embeddings = self.conductor_output(subseq_embeddings)\n\n        # Get the initial states of the decoder\n        h0s_dec = self.tanh(self.fc_init_dec(subseq_embeddings))\n        h0s_dec = h0s_dec.view(self.num_layers, batch_size,\n                               self.num_subsequences, -1).contiguous()\n        # Init the output seq and the first token to 0 tensors\n        out = torch.zeros(batch_size, self.seq_length, self.input_size,\n                          dtype=torch.float, device=device)\n        token = torch.zeros(batch_size, self.subseq_size, self.input_size,\n                            dtype=torch.float, device=device)\n        # Autoregressivly output tokens\n        for sub in range(self.num_subsequences):\n            subseq_embedding = subseq_embeddings[:, sub, :].unsqueeze(1)\n            subseq_embedding = subseq_embedding.expand(\n                -1, self.subseq_size, -1)\n            h0_dec = h0s_dec[:, :, sub, :].contiguous()\n            c0_dec = h0s_dec[:, :, sub, :].contiguous()\n            # Concat the previous token and the current sub embedding as input\n            dec_input = torch.cat((token, subseq_embedding), -1)\n            # Pass through the decoder\n            token, (h0_dec, c0_dec) = self.decoder_RNN(\n                dec_input, (h0_dec, c0_dec))\n            token = self.decoder_output(token)\n            # Fill the out tensor with the token\n            out[:, sub*self.subseq_size: ((sub+1)*self.subseq_size), :] = token\n            # If teacher_forcing replace the output token by the real one sometimes\n            if teacher_forcing:\n                if random.random() <= self.teacher_forcing_ratio:\n                    token = target[:, sub *\n                                   self.subseq_size: ((sub+1)*self.subseq_size), :]\n        return out\n\n\nclass VAE(nn.Module):\n\n    def __init__(self, encoder, decoder, input_representation, teacher_forcing=True, device='cpu'):\n        super(VAE, self).__init__()\n        \"\"\" This initializes the complete VAE \"\"\"\n\n        # Parameters\n        self.input_rep = input_representation\n        self.tf = teacher_forcing\n        self.encoder = nn.ModuleList(encoder)\n        self.decoder = nn.ModuleList(decoder)\n        self.device = device\n\n        # Layers\n        self.hidden_to_mu = nn.Linear(\n            2 * encoder.hidden_size, encoder.latent_size)\n        self.hidden_to_sig = nn.Linear(\n            2 * encoder.hidden_size, encoder.latent_size)\n\n    def forward(self, x):\n\n        if self.input_rep == 'notetuple':\n            batch_size = x[0].size(0)\n        else:\n            batch_size = x.size(0)\n\n        # Encoder pass\n        h_enc, c_enc = self.encoder.init_hidden(batch_size)  # type: ignore\n        hidden = self.encoder(x, h_enc, c_enc, batch_size)\n        # Reparametrization\n        mu = self.hidden_to_mu(hidden)\n        sig = self.hidden_to_sig(hidden)\n        eps = torch.randn_like(mu).detach().to(self.device)\n        latent = (sig.exp().sqrt() * eps) + mu\n\n        # Decoder pass\n        if self.input_rep == 'midilike':\n            # One hot encoding of the target for teacher forcing purpose\n            target = torch.nn.functional.one_hot(x.squeeze(2).long(),\n                                                 self.input_size).float()\n            x_reconst = self.decoder(latent, target, batch_size,\n                                     teacher_forcing=self.tf, device=self.device)\n        else:\n            x_reconst = self.decoder(latent, x, batch_size,\n                                     teacher_forcing=self.tf, device=self.device)\n\n        return mu, sig, latent, x_reconst\n\n    def batch_pass(self, x, loss_fn, optimizer, w_kl, test=False):\n\n        # Zero grad\n        self.zero_grad()\n\n        # Forward pass\n        mu, sig, latent, x_reconst = self(x)\n\n        # Compute losses\n        kl_div = - 0.5 * torch.sum(1 + sig - mu.pow(2) - sig.exp())\n        if self.input_rep in [\"midilike\", \"MVAErep\"]:\n            reconst_loss = loss_fn(x_reconst.permute(\n                0, 2, 1), x.squeeze(2).long())\n        elif self.input_rep == \"notetuple\":\n            x_reconst = x_reconst.permute(0, 2, 1)\n            x_in, l = x\n            loss_ts_maj = loss_fn(\n                x_reconst[:, :len(self.vocab[0]), :],  # type: ignore\n                x_in[:, :, 0].long())\n            current = len(self.vocab[0])  # type: ignore\n\n            loss_ts_min = loss_fn(\n                x_reconst[:, current:current +\n                          len(self.vocab[1]), :],  # type: ignore\n                x_in[:, :, 1].long())\n            current += len(self.vocab[1])  # type: ignore\n\n            loss_pitch = loss_fn(\n                x_reconst[:, current:current + 129, :], x_in[:, :, 2].long())\n            current += 129\n\n            loss_dur_maj = loss_fn(\n                x_reconst[:, current:current +\n                          len(self.vocab[2]), :],  # type: ignore\n                x_in[:, :, 3].long())\n            current += len(self.vocab[2])  # type: ignore\n\n            loss_dur_min = loss_fn(\n                x_reconst[:, current:current +\n                          len(self.vocab[3]), :],  # type: ignore\n                x_in[:, :, 4].long())\n            reconst_loss = loss_ts_maj + loss_ts_min + \\\n                loss_pitch + loss_dur_maj + loss_dur_min\n        else:\n            reconst_loss = loss_fn(x_reconst, x)\n\n        # Backprop and optimize\n        if not test:\n            loss = reconst_loss + (w_kl * kl_div)\n            loss.backward()\n            optimizer.step()\n        else:\n            loss = reconst_loss + kl_div\n\n        return loss, kl_div, reconst_loss\n\n    def generate(self, latent):\n\n        # Create dumb target\n        input_shape = (1, self.decoder.seq_length, self.decoder.input_size)\n        db_trg = torch.zeros(input_shape)  # type: ignore\n        # Forward pass in the decoder\n        generated_bar = self.decoder(latent.unsqueeze(0), db_trg, batch_size=1,\n                                     device=self.device, teacher_forcing=False)\n\n        return generated_bar\n\n\nclass LightningVAE(L.LightningModule):\n\n    def __init__(self, encoder, decoder, input_representation, vocab=None, teacher_forcing=True):\n        super(LightningVAE, self).__init__()\n        \"\"\" This initializes the complete VAE \"\"\"\n        # Parameters\n        self.input_rep = input_representation\n        self.tf = teacher_forcing\n        self.encoder = encoder\n        self.decoder = decoder\n\n        self.w_kl = 0\n\n        self.vocab = vocab\n        if input_representation == 'notetuple' and vocab is None:\n            raise ValueError(\n                'Vocab must be provided for notetuple input representation')\n\n        # Layers\n        self.hidden_to_mu = nn.Linear(\n            2 * encoder.hidden_size, encoder.latent_size)\n        self.hidden_to_sig = nn.Linear(\n            2 * encoder.hidden_size, encoder.latent_size)\n\n        if input_representation in ['pianoroll', 'signallike']:\n            self.loss_fn = torch.nn.MSELoss(reduction='sum')\n        else:\n            self.loss_fn = torch.nn.CrossEntropyLoss(reduction='sum')\n\n        self.save_hyperparameters(ignore=['encoder', 'decoder'])\n\n    def forward(self, x):\n\n        if self.input_rep == 'notetuple':\n            batch_size = x[0].size(0)\n        else:\n            batch_size = x.size(0)\n\n        # Encoder pass\n        h_enc, c_enc = self.encoder.init_hidden(batch_size)\n        hidden = self.encoder(x, h_enc, c_enc, batch_size)\n\n        # Reparametrization\n        mu = self.hidden_to_mu(hidden)\n        sig = self.hidden_to_sig(hidden)\n        eps = torch.randn_like(mu).detach().to(self.device)\n        latent = (sig.exp().sqrt() * eps) + mu\n\n        # Decoder pass\n        if self.input_rep == 'midilike':\n            # One hot encoding of the target for teacher forcing purpose\n            target = torch.nn.functional.one_hot(x.squeeze(2).long(),\n                                                 self.input_size).float()\n            x_reconst = self.decoder(latent, target, batch_size,\n                                     teacher_forcing=self.tf, device=self.device)\n        else:\n            x_reconst = self.decoder(latent, x, batch_size,\n                                     teacher_forcing=self.tf, device=self.device)\n\n        return mu, sig, latent, x_reconst\n\n    def notetuple_reconstruction_loss(self, x_reconst, x):\n        \"\"\"Compute the reconstruction loss for a\n        given input in notetuple format and its reconstruction\"\"\"\n        x_reconst = x_reconst.permute(0, 2, 1)\n        x_in, l = x\n        loss_ts_maj = self.loss_fn(\n            x_reconst[:, :len(self.vocab[0]), :],  # type: ignore\n            x_in[:, :, 0].long())\n        current = len(self.vocab[0])  # type: ignore\n\n        loss_ts_min = self.loss_fn(\n            x_reconst[:, current:current +\n                      len(self.vocab[1]), :],  # type: ignore\n            x_in[:, :, 1].long())\n        current += len(self.vocab[1])  # type: ignore\n\n        loss_pitch = self.loss_fn(\n            x_reconst[:, current:current + 129, :],\n            x_in[:, :, 2].long())\n        current += 129\n\n        loss_dur_maj = self.loss_fn(\n            x_reconst[:, current:current +\n                      len(self.vocab[2]), :],  # type: ignore\n            x_in[:, :, 3].long())\n        current += len(self.vocab[2])  # type: ignore\n\n        loss_dur_min = self.loss_fn(\n            x_reconst[:, current:current +\n                      len(self.vocab[3]), :],  # type: ignore\n            x_in[:, :, 4].long())\n        reconst_loss = loss_ts_maj + loss_ts_min + \\\n            loss_pitch + loss_dur_maj + loss_dur_min\n\n        return reconst_loss\n\n    def compute_reconstruction_loss(self, x, x_reconst):\n        \"\"\" Compute the reconstruction loss for a given input and its reconstruction \"\"\"\n\n        if self.input_rep in [\"midilike\", \"MVAErep\"]:\n            reconst_loss = self.loss_fn(x_reconst.permute(\n                0, 2, 1), x.squeeze(2).long())\n        elif self.input_rep == \"notetuple\":\n            reconst_loss = self.notetuple_reconstruction_loss(x_reconst, x)\n        else:  # pianoroll, signallike\n            reconst_loss = self.loss_fn(x_reconst, x)\n\n        return reconst_loss\n\n    def training_step(self, batch, batch_idx):\n        x = batch\n\n        # Zero grad\n        self.zero_grad()\n\n        # Forward pass\n        mu, sig, _, x_reconst = self(x)\n\n        # Compute losses\n        kl_div = - 0.5 * torch.sum(1 + sig - mu.pow(2) - sig.exp())\n        reconst_loss = self.compute_reconstruction_loss(x, x_reconst)\n\n        # Backprop and optimize\n        loss = reconst_loss + (self.w_kl * kl_div)\n        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n        self.log(\"train_kl_div\", kl_div, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n        self.log(\"train_reconst_loss\", reconst_loss, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n\n        return {\"loss\": loss, \"kl_div\": kl_div, \"reconst_loss\": reconst_loss}\n\n    def validation_step(self, batch, batch_idx):\n        x = batch\n\n        # Zero grad\n        self.zero_grad()\n\n        # Forward pass\n        mu, sig, _, x_reconst = self(x)\n\n        # Compute losses\n        kl_div = - 0.5 * torch.sum(1 + sig - mu.pow(2) - sig.exp())\n        reconst_loss = self.compute_reconstruction_loss(x, x_reconst)\n\n        # Backprop and optimize\n        loss = reconst_loss + kl_div\n        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n        self.log(\"val_kl_div\", kl_div, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n        self.log(\"val_reconst_loss\", reconst_loss, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n\n        return {\"loss\": loss, \"kl_div\": kl_div, \"reconst_loss\": reconst_loss}\n\n    def on_train_epoch_end(self) -> None:\n        \"\"\"\n        Called when the epoch ends.\n        \"\"\"\n        epoch = self.current_epoch\n\n        if self.input_rep in [\"pianoroll\"]:\n            if epoch < 150 and epoch > 0 and epoch % 10 == 0:\n                self.w_kl += 1e-5\n            elif epoch > 150 and epoch % 10 == 0:\n                self.w_kl += 1e-4\n        elif self.input_rep in [\"midilike\", \"signallike\", \"dft128\"] and epoch % 10 == 0 and epoch > 0:\n            self.w_kl += 1e-8\n        elif self.input_rep == \"midimono\" and epoch % 10 == 0 and epoch > 0:\n            self.w_kl += 1e-4\n        elif self.input_rep == \"notetuple\" and epoch % 10 == 0 and epoch > 0:\n            self.w_kl += 1e-6\n\n        self.log(\"w_kl\", self.w_kl, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n        return super().on_train_epoch_end()\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=1e-4)","metadata":{"execution":{"iopub.status.busy":"2023-06-05T16:08:45.407058Z","iopub.execute_input":"2023-06-05T16:08:45.407493Z","iopub.status.idle":"2023-06-05T16:08:45.483401Z","shell.execute_reply.started":"2023-06-05T16:08:45.407458Z","shell.execute_reply":"2023-06-05T16:08:45.482185Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"os.makedirs(f'{output_dr}/{arguments[\"--runname\"]}/models', exist_ok=True)\n\ncallbacks = [\n    L.pytorch.callbacks.ModelCheckpoint(monitor='val_loss',  # type: ignore\n                                        save_top_k=1, mode='min',\n                                        dirpath=f'{output_dr}/{arguments[\"--runname\"]}/models/',\n                                        filename=arguments['--runname'] + \\\n                                        '-{epoch}-{val_loss:.2f}',\n                                        save_last=True),\n    L.pytorch.callbacks.EarlyStopping(monitor='val_loss',  # type: ignore\n                                      patience=5,\n                                      mode='min'),\n    L.pytorch.callbacks.LearningRateMonitor(  # type: ignore\n        logging_interval='step'),\n]\n\ntrainer = L.Trainer(max_epochs=10, default_root_dir=f'{output_dr}/{arguments[\"--runname\"]}/',\n                    enable_checkpointing=True, callbacks=callbacks)\n\nlast_model = f'{output_dr}/{arguments[\"--runname\"]}/models/last.ckpt'\nif os.path.exists(last_model):\n    trainer.fit(model,\n                train_dataloaders=data_loader,\n                val_dataloaders=test_loader,\n                ckpt_path=last_model)\nelse:\n    trainer.fit(model,\n                train_dataloaders=data_loader,\n                val_dataloaders=test_loader)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-06-05T16:08:45.484807Z","iopub.execute_input":"2023-06-05T16:08:45.485860Z","iopub.status.idle":"2023-06-05T16:11:50.808559Z","shell.execute_reply.started":"2023-06-05T16:08:45.485816Z","shell.execute_reply":"2023-06-05T16:11:50.788189Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"INFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\nINFO: Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\nINFO: Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\nINFO: ----------------------------------------------------------------------------------------------------\ndistributed_backend=nccl\nAll distributed processes registered. Starting with 2 processes\n----------------------------------------------------------------------------------------------------\n\n/opt/conda/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /kaggle/working/output/DFT128_01/models exists and is not empty.\n  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\nINFO: Restoring states from the checkpoint path at /kaggle/working/output/DFT128_01/models/last.ckpt\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\nINFO: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\nINFO: \n  | Name          | Type                     | Params\n-----------------------------------------------------------\n0 | encoder       | Encoder_RNN              | 34.7 M\n1 | decoder       | Decoder_RNN_hierarchical | 30.3 M\n2 | hidden_to_mu  | Linear                   | 524 K \n3 | hidden_to_sig | Linear                   | 524 K \n4 | loss_fn       | CrossEntropyLoss         | 0     \n-----------------------------------------------------------\n66.0 M    Trainable params\n0         Non-trainable params\n66.0 M    Total params\n264.107   Total estimated model params size (MB)\nINFO: Restored all states from the checkpoint at /kaggle/working/output/DFT128_01/models/last.ckpt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8ffeecbc4be4fc1aae425c867816f38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50e0ccb3bf6a4d8897eeed39d7e57375"}},"metadata":{}}]},{"cell_type":"code","source":"!cd /kaggle/working\n!zip -r model.zip output/DFT128_01/\n\nfrom IPython.display import FileLink \nFileLink(r'model.zip')","metadata":{"execution":{"iopub.status.busy":"2023-06-05T16:11:50.813001Z","iopub.execute_input":"2023-06-05T16:11:50.813458Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"updating: output/DFT128_01/ (stored 0%)\nupdating: output/DFT128_01/models/ (stored 0%)\nupdating: output/DFT128_01/models/DFT128_01-epoch=5-val_loss=-6961369.50.ckpt","output_type":"stream"}]},{"cell_type":"code","source":"!cd /kaggle/working\n#!rm lightning_logs.zip\n#!rm model.zip\n#!rm -rf output\n#!mkdir output/models/DFT128_01/\n#!ls  output/models/DFT128_01\n#!rm output/models/DFT128_01/last.ckpt\n#!mv output/models/DFT128_01/last-v1.ckpt output/models/DFT128_01/last.ckpt\n\n#!ls  output/models/DFT128_01\n","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]}]}