{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pretty_midi -q\n!pip install torch>=2.0.0 -q\n!pip install tensorboardX -q\n!pip install tbparse -q\n\nimport torch\nprint(torch.__version__)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-17T12:24:21.324160Z","iopub.execute_input":"2023-05-17T12:24:21.324540Z","iopub.status.idle":"2023-05-17T12:25:00.656553Z","shell.execute_reply.started":"2023-05-17T12:24:21.324508Z","shell.execute_reply":"2023-05-17T12:25:00.654812Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m2.0.0+cpu\n","output_type":"stream"}]},{"cell_type":"code","source":"#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Tue Aug 25 13:41:24 2020\n\n@author: prang\n\"\"\"\n\nimport torch  # type: ignore\nfrom torch import nn  # type: ignore\nimport random\n\n\nclass Encoder_RNN(nn.Module):\n\n    def __init__(self, input_dim, hidden_size, latent_size, num_layers,\n                 dropout=0.5, packed_seq=False, device='cpu'):\n        \"\"\" This initializes the encoder \"\"\"\n        super(Encoder_RNN, self).__init__()\n\n        # Parameters\n        self.num_layers = num_layers\n        self.hidden_size = hidden_size\n        self.latent_size = latent_size\n        self.packed_seq = packed_seq\n        self.device = device\n\n        # Layers\n        self.RNN = nn.LSTM(input_dim, hidden_size, batch_first=True,\n                           num_layers=num_layers, bidirectional=True,\n                           dropout=dropout)\n\n    def forward(self, x, h0, c0, batch_size):\n\n        # Pack sequence if needed\n        if self.packed_seq:\n            x = torch.nn.utils.rnn.pack_padded_sequence(x[0], x[1],\n                                                        batch_first=True,\n                                                        enforce_sorted=False)\n        # Forward pass\n        _, (h, _) = self.RNN(x, (h0, c0))\n        # Be sure to not have NaN values\n        assert ((h == h).all()), 'NaN value in the output of the RNN, try to \\\n                                lower your learning rate'\n        h = h.view(self.num_layers, 2, batch_size, -1)\n        h = h[-1]\n        h = torch.cat([h[0], h[1]], dim=1)\n\n        return h\n\n    def init_hidden(self, batch_size=1):\n        # Bidirectional -> num_layers * 2\n        return (torch.zeros(self.num_layers * 2, batch_size, self.hidden_size,\n                            dtype=torch.float, device=self.device),) * 2\n\n\nclass Decoder_RNN_hierarchical(nn.Module):\n\n    def __init__(self, input_size, latent_size, cond_hidden_size, cond_outdim,\n                 dec_hidden_size, num_layers, num_subsequences, seq_length,\n                 teacher_forcing_ratio=0, dropout=0.5):\n        \"\"\" This initializes the decoder \"\"\"\n        super(Decoder_RNN_hierarchical, self).__init__()\n\n        # Parameters\n        self.num_subsequences = num_subsequences\n        self.input_size = input_size\n        self.num_layers = num_layers\n        self.seq_length = seq_length\n        self.teacher_forcing_ratio = teacher_forcing_ratio\n        self.subseq_size = self.seq_length // self.num_subsequences\n\n        # Layers\n        self.tanh = nn.Tanh()\n        self.fc_init_cond = nn.Linear(\n            latent_size, cond_hidden_size * num_layers)\n        self.conductor_RNN = nn.LSTM(latent_size // num_subsequences, cond_hidden_size,\n                                     batch_first=True, num_layers=num_layers,\n                                     bidirectional=False, dropout=dropout)\n        self.conductor_output = nn.Linear(cond_hidden_size, cond_outdim)\n        self.fc_init_dec = nn.Linear(cond_outdim, dec_hidden_size * num_layers)\n        self.decoder_RNN = nn.LSTM(cond_outdim + input_size, dec_hidden_size,\n                                   batch_first=True, num_layers=num_layers,\n                                   bidirectional=False, dropout=dropout)\n        self.decoder_output = nn.Linear(dec_hidden_size, input_size)\n\n    def forward(self, latent, target, batch_size, teacher_forcing, device):\n\n        # Get the initial state of the conductor\n        h0_cond = self.tanh(self.fc_init_cond(latent))\n        h0_cond = h0_cond.view(self.num_layers, batch_size, -1).contiguous()\n        # Divide the latent code in subsequences\n        latent = latent.view(batch_size, self.num_subsequences, -1)\n        # Pass through the conductor\n        subseq_embeddings, _ = self.conductor_RNN(latent, (h0_cond,)*2)\n        subseq_embeddings = self.conductor_output(subseq_embeddings)\n\n        # Get the initial states of the decoder\n        h0s_dec = self.tanh(self.fc_init_dec(subseq_embeddings))\n        h0s_dec = h0s_dec.view(self.num_layers, batch_size,\n                               self.num_subsequences, -1).contiguous()\n        # Init the output seq and the first token to 0 tensors\n        out = torch.zeros(batch_size, self.seq_length, self.input_size,\n                          dtype=torch.float, device=device)\n        token = torch.zeros(batch_size, self.subseq_size, self.input_size,\n                            dtype=torch.float, device=device)\n        # Autoregressivly output tokens\n        for sub in range(self.num_subsequences):\n            subseq_embedding = subseq_embeddings[:, sub, :].unsqueeze(1)\n            subseq_embedding = subseq_embedding.expand(\n                -1, self.subseq_size, -1)\n            h0_dec = h0s_dec[:, :, sub, :].contiguous()\n            c0_dec = h0s_dec[:, :, sub, :].contiguous()\n            # Concat the previous token and the current sub embedding as input\n            dec_input = torch.cat((token, subseq_embedding), -1)\n            # Pass through the decoder\n            token, (h0_dec, c0_dec) = self.decoder_RNN(\n                dec_input, (h0_dec, c0_dec))\n            token = self.decoder_output(token)\n            # Fill the out tensor with the token\n            out[:, sub*self.subseq_size: ((sub+1)*self.subseq_size), :] = token\n            # If teacher_forcing replace the output token by the real one sometimes\n            if teacher_forcing:\n                if random.random() <= self.teacher_forcing_ratio:\n                    token = target[:, sub *\n                                   self.subseq_size: ((sub+1)*self.subseq_size), :]\n        return out\n\n\nclass VAE(nn.Module):\n\n    def __init__(self, encoder, decoder, input_representation, teacher_forcing=True, device='cpu'):\n        super(VAE, self).__init__()\n        \"\"\" This initializes the complete VAE \"\"\"\n        # Parameters\n        self.input_rep = input_representation\n        self.tf = teacher_forcing\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n\n        # Layers\n        self.hidden_to_mu = nn.Linear(\n            2 * encoder.hidden_size, encoder.latent_size)\n        self.hidden_to_sig = nn.Linear(\n            2 * encoder.hidden_size, encoder.latent_size)\n\n    def forward(self, x):\n\n        if self.input_rep == 'notetuple':\n            batch_size = x[0].size(0)\n        else:\n            batch_size = x.size(0)\n\n        # Encoder pass\n        h_enc, c_enc = self.encoder.init_hidden(batch_size)\n        hidden = self.encoder(x, h_enc, c_enc, batch_size)\n        # Reparametrization\n        mu = self.hidden_to_mu(hidden)\n        sig = self.hidden_to_sig(hidden)\n        eps = torch.randn_like(mu).detach().to(self.device)\n        latent = (sig.exp().sqrt() * eps) + mu\n\n        # Decoder pass\n        if self.input_rep == 'midilike':\n            # One hot encoding of the target for teacher forcing purpose\n            target = torch.nn.functional.one_hot(x.squeeze(2).long(),\n                                                 self.input_size).float()\n            x_reconst = self.decoder(latent, target, batch_size,\n                                     teacher_forcing=self.tf, device=self.device)\n        else:\n            x_reconst = self.decoder(latent, x, batch_size,\n                                     teacher_forcing=self.tf, device=self.device)\n\n        return mu, sig, latent, x_reconst\n\n    def batch_pass(self, x, loss_fn, optimizer, w_kl, dataset, test=False):\n\n        # Zero grad\n        self.zero_grad()\n\n        # Forward pass\n        mu, sig, latent, x_reconst = self(x)\n\n        # Compute losses\n        kl_div = - 0.5 * torch.sum(1 + sig - mu.pow(2) - sig.exp())\n        if self.input_rep in [\"midilike\", \"MVAErep\"]:\n            reconst_loss = loss_fn(x_reconst.permute(\n                0, 2, 1), x.squeeze(2).long())\n        elif self.input_rep in [\"pianoroll\", \"signallike\"]:\n            reconst_loss = loss_fn(x_reconst, x)\n        elif self.input_rep == \"notetuple\":\n            x_reconst = x_reconst.permute(0, 2, 1)\n            x_in, l = x\n            loss_ts_maj = loss_fn(\n                x_reconst[:, :len(dataset.vocabs[0]), :], x_in[:, :, 0].long())\n            current = len(dataset.vocabs[0])\n            loss_ts_min = loss_fn(\n                x_reconst[:, current:current+len(dataset.vocabs[1]), :], x_in[:, :, 1].long())\n            current += len(dataset.vocabs[1])\n            loss_pitch = loss_fn(\n                x_reconst[:, current:current + 129, :], x_in[:, :, 2].long())\n            current += 129\n            loss_dur_maj = loss_fn(\n                x_reconst[:, current:current+len(dataset.vocabs[2]), :], x_in[:, :, 3].long())\n            current += len(dataset.vocabs[2])\n            loss_dur_min = loss_fn(\n                x_reconst[:, current:current+len(dataset.vocabs[3]), :], x_in[:, :, 4].long())\n            reconst_loss = loss_ts_maj + loss_ts_min + \\\n                loss_pitch + loss_dur_maj + loss_dur_min\n        else:\n            reconst_loss = loss_fn(x_reconst, x)\n\n        # Backprop and optimize\n        if not test:\n            loss = reconst_loss + (w_kl * kl_div)\n            loss.backward()\n            optimizer.step()\n        else:\n            loss = reconst_loss + kl_div\n\n        return loss, kl_div, reconst_loss\n\n    def generate(self, latent):\n\n        # Create dumb target\n        input_shape = (1, self.decoder.seq_length, self.decoder.input_size)\n        db_trg = torch.zeros(input_shape)\n        # Forward pass in the decoder\n        generated_bar = self.decoder(latent.unsqueeze(0), db_trg, batch_size=1,\n                                     device=self.device, teacher_forcing=False)\n\n        return generated_bar","metadata":{"execution":{"iopub.status.busy":"2023-05-17T12:05:27.403995Z","iopub.execute_input":"2023-05-17T12:05:27.404604Z","iopub.status.idle":"2023-05-17T12:05:27.449547Z","shell.execute_reply.started":"2023-05-17T12:05:27.404574Z","shell.execute_reply":"2023-05-17T12:05:27.447272Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Tue Aug 25 16:28:14 2020\n\n@author: prang\n\"\"\"\n\nimport torch # type: ignore\nimport pretty_midi\nimport os\nimport numpy as np\nfrom operator import attrgetter\nfrom bisect import bisect_left\nimport librosa\nfrom abc import ABC, abstractmethod\n\n#%%\n\n# MIDI extensions\nEXT = ['.mid', '.midi', '.MID', '.MIDI']\n# Primes number\nPRIMES = [43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107,\n          109, 113, 127, 131, 137, 149, 157, 163, 167, 173, 179, 191, 197,\n          211, 223, 227, 233, 239, 251, 257, 263, 269, 277, 281, 293, 307,\n          311, 317, 331, 337, 347, 353, 359, 367, 373, 379, 383, 389, 397,\n          401, 409, 419, 431, 439, 443, 449, 457, 461, 467, 479, 487, 491,\n          499, 503, 509, 521, 541, 547, 557, 563, 569, 577, 587, 593, 599,\n          607, 613, 617, 631, 641, 647, 653, 659, 673, 677, 683, 691, 701,\n          709, 719, 727, 733, 739, 743, 751, 757, 761, 769, 773, 787, 797,\n          809, 821, 827, 839, 853, 857, 863, 877, 881, 887, 907, 911, 919,\n          929, 937, 941, 947, 953, 967, 971, 977, 983, 991, 997, 1009, 1013,\n          1019, 1031, 1039, 1049, 1061, 1069, 1087, 1091, 1097, 1103, 1109,\n          1117, 1123, 1129, 1151, 1163, 1171, 1181, 1187, 1193, 1201, 1213,\n          1217, 1223, 1229, 1237, 1249, 1259, 1277, 1283, 1289, 1297, 1301,\n          1307, 1319, 1327, 1361, 1367, 1373, 1381, 1399, 1409, 1423, 1427,\n          1433, 1439, 1447, 1451, 1459, 1471, 1481, 1487, 1493, 1499, 1511,\n          1523, 1531, 1543, 1549, 1553, 1559, 1567, 1571, 1579, 1583, 1597,\n          1601, 1607, 1613, 1619, 1627, 1637, 1657, 1663, 1667, 1693, 1697,\n          1709, 1721, 1733, 1741, 1747, 1753, 1759, 1777, 1783, 1787, 1801,\n          1811, 1823, 1831, 1847, 1861, 1867, 1871, 1877, 1889, 1901, 1907,\n          1913, 1931, 1949, 1973, 1979, 1987, 1993, 1997, 2003, 2011, 2017,\n          2027, 2039, 2053, 2063]\n\n\n# Usefull functions\ndef takeClosest(myList, myNumber):\n    \"\"\"\n    Assumes myList is sorted. Returns closest value to myNumber.\n\n    If two numbers are equally close, return the smallest number.\n    \"\"\"\n    pos = bisect_left(myList, myNumber)\n    if pos == 0:\n        return myList[0]\n    if pos == len(myList):\n        return myList[-1]\n    before = myList[pos - 1]\n    after = myList[pos]\n    if after - myNumber < myNumber - before:\n       return after\n    else:\n       return before\n\n\n# Abstract class for the different input representations\nclass Representation(ABC):\n\n    def __init__(self, root_dir, nbframe_per_bar=16, mono=False, export=False):\n        \"\"\"\n        Args:\n            root_dir (string) : Path of the directory with all the MIDI files\n            nbframe_per_bar (int) : Number of frame contained in a bar\n            export (bool) : Force the bar to be exported in .pt file or not\n        \"\"\"\n        assert any((fname.endswith(tuple(EXT))) for fname in os.listdir(root_dir)), \"There are no MIDI files in %s\" % root_dir\n        # root directory path which contains the files\n        self.rootdir = root_dir\n        # midi files names\n        self.midfiles = [fname for fname in os.listdir(root_dir) if (fname.endswith(tuple(EXT)))]\n        # Number of frame per bar\n        self.nbframe_per_bar = nbframe_per_bar\n        # Force export or not\n        self.export = export\n        # Monophonic data (separate voices)\n        self.mono = mono\n        # number of tracks contained in the dataset\n        self.nb_tracks = len(self.midfiles)\n\n\n\n    def __len__(self):\n\n        return self.nb_bars # type: ignore\n\n\n\n    def __getitem__(self, index):\n\n        return torch.load(self.prbar_path + '/' + self.barfiles[index]) # type: ignore\n\n\n    @abstractmethod\n    def per_bar_export(self):\n        \"\"\"\n        This function take all the midi files, load them into a pretty_midi object.\n        For a complete documentation of pretty_midi go to :\n            http://craffel.github.io/pretty-midi/\n\n        The midi file is then processed to obtain the given representation of each bar with\n        Finally, it will export each of theses bars in a separate .pt\n        \"\"\"\n        pass\n\n\n\n################################# PIANO-ROLL #################################\n\nclass Pianoroll(Representation):\n\n    def __init__(self, root_dir, nbframe_per_bar=16, mono=False, export=False, save_dir=None):\n        super().__init__(root_dir, nbframe_per_bar=nbframe_per_bar, mono=mono, export=export)\n        if save_dir is None:\n            save_dir = root_dir\n        \n        # Path witch contains the sliced piano-roll\n        if mono:\n            self.prbar_path = save_dir + \"/pianoroll_bar_mono_\" + str(self.nbframe_per_bar)\n        else :\n            self.prbar_path = save_dir + \"/pianoroll_bar_\" + str(self.nbframe_per_bar)\n            \n        if not os.path.exists(self.prbar_path):\n            try:\n                os.mkdir(self.prbar_path)\n            except OSError:\n                print (\"Creation of the directory %s failed\" % self.prbar_path)\n            else:\n                print (\"Successfully created the directory %s \" % self.prbar_path)\n            # Export the piano-roll bat\n            self.per_bar_export()\n        else :\n            if export:\n                self.per_bar_export()\n        # .pt files names\n        self.barfiles = [fname for fname in os.listdir(self.prbar_path) if fname.endswith('.pt')]\n        # total number of bars\n        self.nb_bars = len(self.barfiles)\n\n\n\n    def sliced_and_save_pianoroll(self, pianoroll, downbeats, fs, num_bar):\n\n        for i in range(len(downbeats)-1):\n            sp = pianoroll[:,int(round(downbeats[i]*fs)):int(round(downbeats[i+1]*fs))-1]\n            if sp.shape[1] > 256:\n                sp = sp[:,0:256]\n            elif sp.shape[1] < 256 and sp.shape[1] > 0:\n                sp = np.pad(sp,((0,0),(0,256 - sp.shape[1])), 'edge')\n            if sp.shape[1] > 0 :\n                # downsample\n                sp = sp[:,::int(256/self.nbframe_per_bar)]\n                # convert to tensor\n                sp = torch.Tensor(sp)\n                assert (sp.shape[1]==self.nbframe_per_bar), \"Error, a piano-roll have the wrong size : %s\" % sp.shape[1]\n                # binarize\n                sp[sp!=0]=1\n                # Save the tensor\n                torch.save(sp.permute(1,0), self.prbar_path + \"/prbar\" + str(num_bar) +  \".pt\")\n                num_bar += 1\n\n        return num_bar\n\n\n\n    def per_bar_export(self):\n\n        num_error = 0\n        num_bar = 0\n        # load each .mid file in a pretty_midi object\n        for index in range(len(self.midfiles)):\n            try:\n                midi_data = pretty_midi.PrettyMIDI(self.rootdir + '/' + self.midfiles[index])\n                downbeats = midi_data.get_downbeats()\n                fs = 257 / (midi_data.get_end_time() / len(downbeats))\n                # If monophonic data is required, we separate each voice\n                if self.mono:\n                    for inst in midi_data.instruments:\n                        if not inst.is_drum:\n                            pianoroll = inst.get_piano_roll(fs=fs)\n                            num_bar = self.sliced_and_save_pianoroll(pianoroll, downbeats, fs, num_bar)\n                else :\n                    pianoroll = midi_data.get_piano_roll(fs=fs) # type: ignore\n                    num_bar = self.sliced_and_save_pianoroll(pianoroll, downbeats, fs, num_bar)\n            except KeyError:\n                num_error += 1\n        print(\"total number of file : \", len(self.midfiles))\n        print('num error : ', num_error)\n\n\n\n################################# MIDI-like ##################################\n\nclass Midilike(Representation):\n\n    def __init__(self, root_dir, nbframe_per_bar=16, mono=False, export=False, save_dir=None):\n        super().__init__(root_dir, nbframe_per_bar=nbframe_per_bar, mono=mono, export=export)\n        \n        if save_dir is None:\n            save_dir = root_dir\n        \n        # One hot encoding of the vocabulary\n        self.vocabulary = self.get_vocab_encoding()\n        # Path witch contains the sliced piano-roll\n        self.prbar_path = save_dir + \"/MIDIlike_bar\"\n        if not os.path.exists(self.prbar_path):\n            try:\n                os.mkdir(self.prbar_path)\n            except OSError:\n                print (\"Creation of the directory %s failed\" % self.prbar_path)\n            else:\n                print (\"Successfully created the directory %s \" % self.prbar_path)\n            # Export the piano-roll bat\n            self.per_bar_export()\n        else :\n            if export:\n                self.per_bar_export()\n        # .pt files names\n        self.barfiles = [fname for fname in os.listdir(self.prbar_path) if fname.endswith('.pt')]\n        # total number of bars\n        self.nb_bars = len(self.barfiles)\n\n\n\n    def get_vocab_encoding(self):\n        \"\"\"\n        Return a dictionnary with the corresponding indexes of every word contained in\n        the vocabulary (one hot encoding).\n\n        e.g : vocab = {'NOTE_ON<1>' : 0\n                       'NOTE_ON<2>' : 1\n                           ...\n                       'TIME_SHIFT<1> : 128\n                           ...             }\n        \"\"\"\n        vocab = {}\n        current_ind = 0\n\n        # All the NOTE_ON events\n        rootstr = \"NOTE_ON<\"\n        for i in range(0, 128):\n            event = rootstr + str(i) + '>'\n            vocab[event] = current_ind\n            current_ind += 1\n        # All the NOTE_OFF events\n        rootstr = \"NOTE_OFF<\"\n        for i in range(0, 128):\n            event = rootstr + str(i) + '>'\n            vocab[event] = current_ind\n            current_ind += 1\n        # All the TIME_SHIFT events\n        rootstr = \"TIME_SHIFT<\"\n        for i in range(10, 1001, 10):\n            event = rootstr + str(i) + '>'\n            vocab[event] = current_ind\n            current_ind += 1\n        # All the SET_VELOCITY events\n        rootstr = \"SET_VELOCITY<\"\n        for i in range(0,128,4):\n            event = rootstr + str(i) + '>'\n            vocab[event] = current_ind\n            current_ind += 1\n        # The NOTHING event\n        vocab['NOTHING'] = current_ind\n\n        return vocab\n\n\n\n    def string_representation(self, Vinst):\n        \"\"\"\n        Return the representation with string (\"NOTE_ON<56>, ...) from the\n        corresponding integer representation Vinst (list of int)\n        \"\"\"\n        str_rep = []\n        for i in Vinst:\n            str_rep.append(list(self.vocabulary.keys())[list(self.vocabulary.values()).index(int(i))])\n\n        return str_rep\n\n\n\n    def per_bar_export(self):\n        \"\"\"\n        This function take all the midi files, load them into a pretty_midi object.\n        For a complete documentation of pretty_midi go to :\n            http://craffel.github.io/pretty-midi/\n\n        The midi file is then processed to obtain a MIDI-like event-based representation.\n        More info on this representation here :\n            https://arxiv.org/pdf/1809.04281.pdf\n\n        Ex :  SET_VELOCITY<80>, NOTE_ON<60>\n              TIME_SHIFT<500>, NOTE_ON<64>\n              TIME_SHIFT<500>, NOTE_ON<67>\n              TIME_SHIFT<1000>, NOTE_OFF<60>, NOTE_OFF<64>, NOTE_OFF<67>\n              TIME_SHIFT<500>, SET_VELOCITY<100>, NOTE_ON<65>\n              TIME_SHIFT<500>, NOTE_OFF<65>\n\n        Finally, it will export each of theses bars in a separate .pt\n        \"\"\"\n        # number of error with the key analyzer\n        num_error = 0\n        # array of all the tensor representing each bar\n        all_bars = []\n        # load each .mid file in a pretty_midi object\n        for index in range(len(self.midfiles)):\n            try:\n                midi_data = pretty_midi.PrettyMIDI(self.rootdir + '/' + self.midfiles[index])\n                downbeats = midi_data.get_downbeats() #start_time = midi_data.estimate_beat_start()\n                current_velocity = 64.\n                # Possible value for the velocity\n                velocity_list = [i for i in range(0,128,4)]\n                # Possible value for the time shifts\n                timeshift_list = [i for i in range(10,1001,10)]\n                for i in range(len(downbeats)-1):\n                    list_notes = []\n                    V = []\n                    for inst in midi_data.instruments:\n                        if not inst.is_drum:\n                            for n in inst.notes:\n                                if (n.start < downbeats[i+1] and n.end >= downbeats[i]):\n                                    list_notes.append(n)\n                    # Sort list by pitch\n                    list_notes = sorted(list_notes, key = lambda i: i.pitch, reverse = False)\n                    if len(list_notes)==0:\n                        gap = (downbeats[i+1] - downbeats[i])*1000\n                        while(gap > 1000):\n                            V.append(self.vocabulary['TIME_SHIFT<1000>'])\n                            gap = gap - 1000\n                        timeshift = takeClosest(timeshift_list, gap)\n                        V.append(self.vocabulary['TIME_SHIFT<' + str(timeshift) + '>'])\n                    else:\n                        # iterate over list_notes to construct the representation\n                        current_time = downbeats[i]\n                        while(list_notes):\n                            closest_note_on = min(list_notes, key=attrgetter('start'))\n                            closest_note_off = min(list_notes, key=attrgetter('end'))\n                            if closest_note_off.end > closest_note_on.start:\n                                gap = (closest_note_on.start - current_time)*1000\n                                if gap > timeshift_list[0]/2:\n                                    while(gap > 1000):\n                                        V.append(self.vocabulary['TIME_SHIFT<1000>'])\n                                        gap = gap - 1000\n                                    timeshift = takeClosest(timeshift_list, gap)\n                                    V.append(self.vocabulary['TIME_SHIFT<' + str(timeshift) + '>'])\n                                if takeClosest(velocity_list, closest_note_on.velocity) != current_velocity:\n                                    veloc = takeClosest(velocity_list, closest_note_on.velocity)\n                                    V.append(self.vocabulary['SET_VELOCITY<' + str(veloc) + '>'])\n                                    current_velocity = veloc\n                                V.append(self.vocabulary['NOTE_ON<' + str(closest_note_on.pitch) + '>'])\n                                if closest_note_on.start > current_time:\n                                    current_time = closest_note_on.start\n                                if closest_note_on.end > downbeats[i+1]:\n                                    list_notes.remove(closest_note_on)\n                                else :\n                                    # Set a value > end to start to not taking it in account anymore\n                                    closest_note_on.start = closest_note_on.end + 10\n                            else :\n                                gap = (closest_note_off.end - current_time)*1000\n                                if gap > timeshift_list[0]/2:\n                                    while(gap > 1000):\n                                        V.append(self.vocabulary['TIME_SHIFT<1000>'])\n                                        gap = gap - 1000\n                                    timeshift = takeClosest(timeshift_list, gap)\n                                    V.append(self.vocabulary['TIME_SHIFT<' + str(timeshift) + '>'])\n                                V.append(self.vocabulary['NOTE_OFF<' + str(closest_note_off.pitch) + '>'])\n                                current_time = closest_note_off.end\n                                list_notes.remove(closest_note_off)\n                    # Store the tensor in all_bars\n                    all_bars.append(torch.tensor(V))\n            except KeyError:\n                num_error += 1\n        print('num error : ', num_error)\n        # Cleaning of the tensor : supressing ones with more than 160 events\n        # and padding to have a constant size equal to 160\n        empty_bar = False\n        total_num = 0\n        for i, vec in enumerate(all_bars):\n            # add the empty bar only one time\n            if len(vec) == 1:\n                if not empty_bar:\n                    clean_vec = torch.tensor([self.vocabulary['NOTHING']]*64)\n                    clean_vec[0] = vec\n                    torch.save(clean_vec.unsqueeze(1), self.prbar_path + \"/Mlikebar_\" + str(i) + \".pt\")\n                    empty_bar = True\n                    total_num += 1\n            elif len(vec) < 64:\n                clean_vec = torch.tensor([self.vocabulary['NOTHING']]*64)\n                clean_vec[:len(vec)]=vec\n                torch.save(clean_vec.unsqueeze(1), self.prbar_path + \"/Mlikebar_\" + str(i) + \".pt\")\n                total_num += 1\n            elif len(vec) == 64:\n                torch.save(vec.unsqueeze(1), self.prbar_path + \"/Mlikebar_\" + str(i) + \".pt\")\n                total_num += 1\n        print(\"Initial number of bar : {}\\n \\\n               After cleaning : {}\\n \\\n               Number of suppression : {}\".format(len(all_bars), total_num, len(all_bars) - total_num))\n\n\n\n\n############################### MIDI-like mono ###############################\n\nclass Midimono(Representation):\n\n    def __init__(self, root_dir, nbframe_per_bar=16, mono=True, export=False, save_dir=None):\n        super().__init__(root_dir, nbframe_per_bar=nbframe_per_bar, mono=mono, export=export)\n        \n        if save_dir is None:\n            save_dir = root_dir\n            \n        # Path witch contains the sliced piano-roll\n        self.prbar_path = save_dir + \"/MIDIMono_bar\"\n        if not os.path.exists(self.prbar_path):\n            try:\n                os.mkdir(self.prbar_path)\n            except OSError:\n                print (\"Creation of the directory %s failed\" % self.prbar_path)\n            else:\n                print (\"Successfully created the directory %s \" % self.prbar_path)\n            # Export the piano-roll bat\n            self.per_bar_export()\n        else :\n            if export:\n                self.per_bar_export()\n        # .pt files names\n        self.barfiles = [fname for fname in os.listdir(self.prbar_path) if fname.endswith('.pt')]\n        # total number of bars\n        self.nb_bars = len(self.barfiles)\n\n\n\n    def get_polyphonic_bars(self, pr_dataset):\n\n        indices = set()\n        for i in range(len(pr_dataset)):\n            for j,frame in enumerate(pr_dataset[i]):\n                if frame.nonzero().nelement() > 1:\n                    indices.add(i)\n\n        return indices\n\n\n\n    def to_pianoroll(self, v):\n\n        pianoroll = torch.zeros(16, 128)\n        current_note = -1\n        for i,e in enumerate(v):\n            if e < 128:\n                pianoroll[i, int(e)] = 1\n                current_note = int(e)\n            elif e == 128:\n                if current_note != 129:\n                    pianoroll[i, current_note] = 1\n\n        return pianoroll\n\n\n\n    def per_bar_export(self):\n\n        PR = Pianoroll(self.rootdir, nbframe_per_bar=16, mono=True)\n        poly_bars = self.get_polyphonic_bars(PR)\n        num_vec = 0\n        for i in range(len(PR)):\n            if i not in poly_bars:\n                vec = torch.zeros(16)\n                current_note = -1\n                for j,frame in enumerate(PR[i]):\n                    if frame.nonzero().nelement() == 0:\n                        if current_note != 129 and current_note != -1:\n                            # note_off event\n                            vec[j] = 129\n                            current_note = 129\n                        else :\n                            # rest event\n                            vec[j] = 128\n                            if current_note == -1:\n                                current_note = 129\n                    else :\n                        if current_note == int(frame.nonzero()):\n                            # rest event\n                            vec[j] = 128\n                        else :\n                            # note_on event\n                            vec[j] = int(frame.nonzero())\n                            current_note = int(frame.nonzero())\n                # Save the tensor\n                torch.save(vec.unsqueeze(1), self.prbar_path + \"/MVAEbar_\" + str(num_vec) + \".pt\")\n                num_vec += 1\n\n\n\n################################# NoteTuple ##################################\n\nclass Notetuple(Representation):\n\n    def __init__(self, root_dir, nbframe_per_bar=16, mono=False, export=False, save_dir=None):\n        super().__init__(root_dir, nbframe_per_bar=nbframe_per_bar, mono=mono, export=export)\n        \n        if save_dir is None:\n            save_dir = root_dir\n        \n        # vocabs\n        self.vocabs = self.get_vocabs_encoding()\n        self.ts_major = self.vocabs[0]\n        self.ts_minor = self.vocabs[1]\n        self.dur_major = self.vocabs[2]\n        self.dur_minor = self.vocabs[3]\n        # Path witch contains the sliced piano-roll\n        self.prbar_path = save_dir + \"/NoteTuple_bar\"\n        if not os.path.exists(self.prbar_path):\n            try:\n                os.mkdir(self.prbar_path)\n            except OSError:\n                print (\"Creation of the directory %s failed\" % self.prbar_path)\n            else:\n                print (\"Successfully created the directory %s \" % self.prbar_path)\n            # Export the bar\n            self.per_bar_export()\n        else :\n            if export:\n                self.per_bar_export()\n        # .pt files names\n        self.barfiles = [fname for fname in os.listdir(self.prbar_path) if fname.endswith('.pt')]\n        # total number of bars\n        self.nb_bars = len(self.barfiles)\n\n\n\n    def get_vocabs_encoding(self):\n        # timeshift major_ticks_vocab\n        ts_major = {}\n        ind = 0\n        for val in [i for i in range(0,9601,800)]:\n            ts_major[val] = ind\n            ind += 1\n        ts_major[-1] = ind\n        # timeshift minor_ticks_vocab\n        ts_minor = {}\n        ind = 0\n        for val in [i for i in range(0,800,10)]:\n            ts_minor[val] = ind\n            ind += 1\n        ts_minor[-1] = ind\n        # duration major_ticks_vocab\n        dur_major = {}\n        ind = 0\n        for val in [i for i in range(0,9501,500)]:\n            dur_major[val] = ind\n            ind += 1\n        dur_major[-1] = ind\n        # duration minor_ticks_vocab\n        dur_minor = {}\n        ind = 0\n        for val in [i for i in range(0,500,10)]:\n            dur_minor[val] = ind\n            ind += 1\n        dur_minor[-1] = ind\n        return ts_major, ts_minor, dur_major, dur_minor\n\n\n\n    def value_to_class(self, bar):\n        # Change the value of the timeshift and duration to a class number\n        for i,tupl in enumerate(bar):\n            for j,v in enumerate(tupl):\n                if j == 0:\n                    bar[i][j] = self.ts_major[int(v)]\n                if j == 1:\n                    bar[i][j] = self.ts_minor[int(v)]\n                if j == 2:\n                    if v == -1:\n                       bar[i][j] = 128\n                if j == 3:\n                    bar[i][j] = self.dur_major[int(v)]\n                if j == 4:\n                    bar[i][j] = self.dur_minor[int(v)]\n        return bar\n\n\n\n    def class_to_value(self, bar):\n        # Change the class number of the timeshift and duration to the real value\n        for i,tupl in enumerate(bar):\n            for j,v in enumerate(tupl):\n                if j == 0:\n                    bar[i][j] = list(self.ts_major.keys())[list(self.ts_major.values()).index(int(v))]\n                if j == 1:\n                    bar[i][j] = list(self.ts_minor.keys())[list(self.ts_minor.values()).index(int(v))]\n                if j == 2:\n                    if v == 128:\n                        bar[i][j] = -1\n                if j == 3:\n                    bar[i][j] = list(self.dur_major.keys())[list(self.dur_major.values()).index(int(v))]\n                if j == 4:\n                    bar[i][j] = list(self.dur_minor.keys())[list(self.dur_minor.values()).index(int(v))]\n        return bar\n\n\n\n    def per_bar_export(self):\n        num_error = 0\n        # to store all the bars\n        all_bars = []\n        for index in range(len(self.midfiles)):\n            try:\n                # load each .mid file in a pretty_midi object\n                midi_data = pretty_midi.PrettyMIDI(self.rootdir + '/' + self.midfiles[index])\n                downbeats = midi_data.get_downbeats() #start_time = midi_data.estimate_beat_start()\n                # Possible value for the time shifts (from 0 to 10s)\n                # 13 major ticks\n                timeshift_major_ticks = [i for i in range(0,9601,800)]\n                # 77 minor ticks\n                timeshift_minor_ticks = [i for i in range(0,800,10)]\n                # Possible value for the duration\n                # 13 major ticks\n                dur_major_ticks = [i for i in range(0,9501,500)]\n                # 77 minor ticj=ks\n                dur_minor_ticks = [i for i in range(0,500,10)]\n                for i in range(len(downbeats)-1):\n                    list_notes = []\n                    V = []\n                    for inst in midi_data.instruments:\n                        if not inst.is_drum:\n                            for n in inst.notes:\n                                if (n.start < downbeats[i+1] and n.start >= downbeats[i]):\n                                    list_notes.append(n)\n                    list_notes = sorted(list_notes, key = lambda i: i.pitch, reverse = False)\n                    # iterate over list_notes to construct the representation\n                    current_time = downbeats[i]\n                    while(list_notes):\n                        closest_note_on = min(list_notes, key=attrgetter('start'))\n                        time_shift = (closest_note_on.start - current_time)*1000\n                        tmat = timeshift_major_ticks[int(time_shift//800)]\n                        tmit = timeshift_minor_ticks[int((time_shift%800)//10)]\n                        duration = (closest_note_on.end - closest_note_on.start)*1000\n                        dmat = dur_major_ticks[int(duration//500)]\n                        dmit = dur_minor_ticks[int((duration%500)//10)]\n                        current_time = closest_note_on.start\n                        V.append((tmat,tmit,closest_note_on.pitch, dmat, dmit))\n                        list_notes.remove(closest_note_on)\n                    # Store the tensor in all_bars\n                    all_bars.append(torch.tensor(V))\n            except KeyError:\n                num_error += 1\n        print('num error : ', num_error)\n        # Save all tensor\n        total_num = 0\n        for i, vec in enumerate(all_bars):\n            if len(vec) < 32 and len(vec) > 0:\n                clean_vec = torch.zeros(32,5).fill_(-1)\n                clean_vec[:len(vec)] = vec\n                clean_vec = self.value_to_class(clean_vec)\n                torch.save((clean_vec, len(vec)), self.prbar_path + \"/Ntuplebar\" + str(i) + \".pt\")\n                total_num += 1\n            elif len(vec) == 32:\n                vec = self.value_to_class(vec)\n                torch.save((clean_vec, len(vec)), self.prbar_path + \"/Ntuplebar\" + str(i) + \".pt\")\n                total_num += 1\n        print(\"Initial number of bar : {}\\n \\\n               After cleaning : {}\\n \\\n               Number of suppression : {}\".format(len(all_bars), total_num, len(all_bars) - total_num))\n\n\n\n\n################################# Signal-like ################################\n\nclass Signallike(Representation):\n\n    def __init__(self, root_dir, nbframe_per_bar=16, mono=False, export=False, save_dir=None):\n        super().__init__(root_dir, nbframe_per_bar=nbframe_per_bar, mono=mono, export=export)\n        \n        if save_dir is None:\n            save_dir = root_dir\n        \n        # Path to export the .pt files\n        if self.mono:\n            self.prbar_path = save_dir + \"/Signallike_bar_mono_\" + str(self.nbframe_per_bar)\n        else:\n            self.prbar_path = save_dir + \"/Signallike_bar_\" + str(self.nbframe_per_bar)\n            \n        if not os.path.exists(self.prbar_path):\n            try:\n                os.mkdir(self.prbar_path)\n            except OSError:\n                print (\"Creation of the directory %s failed\" % self.prbar_path)\n            else:\n                print (\"Successfully created the directory %s \" % self.prbar_path)\n            # Export the piano-roll bat\n            self.per_bar_export()\n        else :\n            if export:\n                self.per_bar_export()\n        # .pt files names\n        self.barfiles = [fname for fname in os.listdir(self.prbar_path) if fname.endswith('.pt')]\n        # total number of bars\n        self.nb_bars = len(self.barfiles)\n        # Size of the signal representation\n        self.signal_size = len(self.__getitem__(0).flatten())\n\n\n\n    def back_to_pianoroll(self, V):\n        \"\"\"\n        Inverse the process : get a piano-roll from a signal-like representation V.\n        \"\"\"\n        PR = ((np.abs(librosa.core.stft(V , n_fft=2048, window='blackman'))))[PRIMES[:128]]\n        return abs(PR)\n\n\n\n    def get_polyphonic_bars(self, pr_dataset):\n        indices = set()\n        for i in range(len(pr_dataset)):\n            for j,frame in enumerate(pr_dataset[i]):\n                if frame.nonzero().nelement() > 1:\n                    indices.add(i)\n        return indices\n\n\n\n    def per_bar_export(self):\n        \"\"\"\n        This function take the self.midfiles[index], load it into a pretty_midi object.\n        For a complete documentation of pretty_midi go to :\n            http://craffel.github.io/pretty-midi/\n\n        The midi file is then processed with stft to obtain a signal-like representation\n        and exported in a .pt file.\n        \"\"\"\n        PR = Pianoroll(self.rootdir, nbframe_per_bar=self.nbframe_per_bar, mono=self.mono)\n        if self.mono:\n            poly_bars = self.get_polyphonic_bars(PR)\n        else:\n            poly_bars = []\n        for i in range(len(PR)):\n            if i not in poly_bars:\n                final_vals = np.zeros((1025, PR[i].permute(1,0).shape[1])).astype(complex)\n                final_vals[PRIMES[:128], :] = np.array(PR[i].permute(1,0)) + 1j * ((np.array(PR[i].permute(1,0)) > 0))\n                V = torch.Tensor(librosa.core.istft(final_vals, window='blackman'))\n                torch.save(V.reshape(64, -1), self.prbar_path + \"/Slikebar_\" + str(i) + \".pt\")","metadata":{"execution":{"iopub.status.busy":"2023-05-17T12:05:27.452297Z","iopub.execute_input":"2023-05-17T12:05:27.452664Z","iopub.status.idle":"2023-05-17T12:05:27.585486Z","shell.execute_reply.started":"2023-05-17T12:05:27.452631Z","shell.execute_reply":"2023-05-17T12:05:27.583223Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Usefull functions\ndef increase_wkl(epoch, w_kl, input_rep):\n\n    if input_rep == \"pianoroll\":\n        if epoch < 150  and epoch > 0:\n            if epoch % 10 == 0:\n                w_kl += 1e-5\n        else :\n            if epoch % 10 == 0 :\n                w_kl += 1e-4\n    elif input_rep in [\"midilike\", \"signallike\"]:\n        if epoch % 10 == 0 and epoch > 0:\n            w_kl += 1e-8\n    elif input_rep == \"midimono\":\n        if epoch % 10 == 0 and epoch > 0:\n            w_kl += 1e-4\n    elif input_rep == \"notetuple\":\n        if epoch % 10 == 0 and epoch > 0:\n            w_kl += 1e-6\n\n    return w_kl","metadata":{"execution":{"iopub.status.busy":"2023-05-17T12:05:27.589170Z","iopub.execute_input":"2023-05-17T12:05:27.589651Z","iopub.status.idle":"2023-05-17T12:05:27.603548Z","shell.execute_reply.started":"2023-05-17T12:05:27.589608Z","shell.execute_reply":"2023-05-17T12:05:27.600988Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"arguments = {\n    '--path': '/kaggle/input/jsb-chorales-signallike-embeddings/dataset',\n    '--save_dt': '/kaggle/working/dataset',\n    '--o': '/kaggle/working/output',\n    '--runname': 'PIANOROLL_01',\n    '--save': True,\n\n    '--gpu': True if torch.cuda.is_available() else False,\n    '--mps': True if torch.backends.mps.is_available() else False,\n    '--gpudev': 0,\n\n    '--lr': 1e-4,\n    '--bsize': 16,\n    '--nbframe': 16,\n    '--inputrep': 'pianoroll',\n    '--maxiter': 10,\n}\n\nprint(arguments)","metadata":{"execution":{"iopub.status.busy":"2023-05-17T12:25:22.111716Z","iopub.execute_input":"2023-05-17T12:25:22.112161Z","iopub.status.idle":"2023-05-17T12:25:22.123890Z","shell.execute_reply.started":"2023-05-17T12:25:22.112123Z","shell.execute_reply":"2023-05-17T12:25:22.118817Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"{'--path': '/kaggle/input/jsb-chorales-signallike-embeddings/dataset', '--save_dt': '/kaggle/working/dataset', '--o': '/kaggle/working/output', '--runname': 'PIANOROLL_01', '--save': True, '--gpu': False, '--mps': False, '--gpudev': 0, '--lr': 0.0001, '--bsize': 16, '--nbframe': 16, '--inputrep': 'pianoroll', '--maxiter': 10}\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch # type: ignore\nfrom tensorboardX import SummaryWriter # type: ignore\nfrom tbparse import SummaryReader # type: ignore\nfrom tqdm import tqdm\nimport os\n\n# Set GPU device and backend\nif arguments['--gpu']:\n    torch.backends.cudnn.benchmark = True\n    torch.cuda.set_device(int(arguments['--gpudev']))\n\n# Set detect anomaly\ntorch.autograd.set_detect_anomaly(True)\n\n# Parameters\ntrain_path = arguments['--path'] + '/train'\ntest_path = arguments['--path'] + '/test'\nbatch_size = int(arguments['--bsize'])\nnb_frame = int(arguments['--nbframe'])\nif arguments['--o'] == 'None':\n    output_dr = os.getcwd() + '/output'\nelse :\n    output_dr = arguments['--o']\n    \nsave_dir_train = f\"{arguments['--save_dt']}/train\"\nos.makedirs(save_dir_train, exist_ok=True)\nsave_dir_test = f\"{arguments['--save_dt']}/test\"\nos.makedirs(save_dir_test, exist_ok=True)\n\n# load the dataset\nif arguments['--inputrep']==\"pianoroll\":\n    dataset = Pianoroll(train_path, nbframe_per_bar=nb_frame, save_dir=save_dir_train)\n    testset = Pianoroll(test_path, nbframe_per_bar=nb_frame, save_dir=save_dir_test)\n    input_dim = 128\n    seq_length = nb_frame\nelif arguments['--inputrep']==\"midilike\":\n    dataset = Midilike(train_path, save_dir=save_dir_train)\n    testset = Midilike(test_path, save_dir=save_dir_test)\n    input_dim = 1\nelif arguments['--inputrep']==\"midimono\":\n    dataset = Midimono(train_path, save_dir=save_dir_train)\n    testset = Midimono(test_path, save_dir=save_dir_test)\n    input_dim = 1\nelif arguments['--inputrep']==\"signallike\":\n    dataset = Signallike(train_path, nbframe_per_bar=nb_frame*2, mono=True, save_dir=save_dir_train)\n    testset = Signallike(test_path, nbframe_per_bar=nb_frame*2, mono=True, save_dir=save_dir_test)\n    input_dim = dataset.signal_size//64\nelif arguments['--inputrep']==\"notetuple\":\n    dataset = Notetuple(train_path, save_dir=save_dir_train)\n    testset = NoteTupleRepresentation(test_path, save_dir=save_dir_test) # type: ignore\n    input_dim = 5\nelse :\n    raise NotImplementedError(\"Representation {} not implemented\".format(arguments['--inputrep']))\n\n# Init the dataloader\ndata_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=1,\n                                        pin_memory=True, shuffle=True, drop_last=True)\ntest_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, num_workers=1,\n                                        pin_memory=True, shuffle=True, drop_last=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-17T12:25:29.904092Z","iopub.execute_input":"2023-05-17T12:25:29.904440Z","iopub.status.idle":"2023-05-17T12:25:39.337333Z","shell.execute_reply.started":"2023-05-17T12:25:29.904417Z","shell.execute_reply":"2023-05-17T12:25:39.336095Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"device = 'cpu'\n\n# Set GPU device and backend\nif arguments['--gpu']:\n    torch.backends.cudnn.benchmark = True  # type: ignore\n    torch.cuda.set_device(int(arguments['--gpudev']))\n    device = 'cuda'\n\nif arguments['--mps']:\n    device = 'mps'\n\n# init writer for tensorboard\nwriter = SummaryWriter(f'{output_dr}/runs/{arguments[\"--runname\"]}')\n\n# Model parameters\nenc_hidden_size = 1024\ncond_hidden_size = 1024\ndec_hidden_size = 1024\ncond_outdim = 512\nnum_layers_enc = 2\nnum_layers_dec = 2\nnum_subsequences = 4\nlatent_size = 256\nif arguments['--inputrep'] in ['pianoroll', 'signallike']:\n    output_dim = input_dim\n    if arguments['--inputrep']=='pianoroll':\n        seq_length = 16\n    else :\n        seq_length = 64\nelif arguments['--inputrep']==\"midilike\":\n    output_dim = len(dataset.vocabulary) # type: ignore\n    seq_length = 64\nelif arguments['--inputrep']==\"midimono\":\n    output_dim = 130\n    seq_length = 16\nelif arguments['--inputrep']==\"notetuple\":\n    output_dim = sum([len(v) for v in dataset.vocabs]) + 129 # type: ignore\n    seq_length = 32\n\n# Instanciate model\nencoder = Encoder_RNN(input_dim, enc_hidden_size,\n                        latent_size, num_layers_enc, device=device)\ndecoder = Decoder_RNN_hierarchical(output_dim, latent_size, cond_hidden_size,  # type: ignore\n                                     cond_outdim, dec_hidden_size, num_layers_dec,\n                                     num_subsequences, seq_length)  # type: ignore\nmodel = VAE(encoder, decoder, arguments['--inputrep'], device=device)\n\n#### Retrieve the model ####\nimport glob \npossible_models = glob.glob(f\"{output_dr}/models/{arguments['--runname']}*.pth\")\nstart_epoch = 0\nif len(possible_models) > 0:\n  weights = sorted(possible_models, reverse=True)[0]\n  model.load_state_dict(torch.load(weights))\n    \n    \n  if os.path.isdir(f'{output_dr}/runs/{arguments[\"--runname\"]}/data_Losses_test'):            \n      reader = SummaryReader(f'{output_dr}/runs/{arguments[\"--runname\"]}/data_Losses_test')\n      df = reader.scalars\n      start_epoch = df['step'].max()\n  else:\n      start_epoch = weights.split('_')[-1][:-4]\n\nprint(start_epoch)","metadata":{"execution":{"iopub.status.busy":"2023-05-17T12:26:14.721316Z","iopub.execute_input":"2023-05-17T12:26:14.721771Z","iopub.status.idle":"2023-05-17T12:26:15.399917Z","shell.execute_reply.started":"2023-05-17T12:26:14.721732Z","shell.execute_reply":"2023-05-17T12:26:15.398459Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"4\n","output_type":"stream"}]},{"cell_type":"code","source":"# Loss\nif arguments['--inputrep'] in ['pianoroll', 'signallike']:\n    loss_fn = torch.nn.MSELoss(reduction='sum')\nelse:\n    loss_fn = torch.nn.CrossEntropyLoss(reduction='sum')\n\nloss_fn = loss_fn.to(device=device)  # type: ignore\nmodel = model.to(device=device)  # type: ignore\n\n# Optimizer\noptimizer = torch.optim.Adam(\n    model.parameters(), lr=float(arguments['--lr']))","metadata":{"execution":{"iopub.status.busy":"2023-05-17T12:26:19.763281Z","iopub.execute_input":"2023-05-17T12:26:19.763640Z","iopub.status.idle":"2023-05-17T12:26:19.770739Z","shell.execute_reply.started":"2023-05-17T12:26:19.763609Z","shell.execute_reply":"2023-05-17T12:26:19.770006Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Start training\nloss_test_min_reconst = 10e6\nw_kl = 0\n\nfor epoch in range(int(start_epoch), int(arguments['--maxiter'])):\n    print('epoch : ' + str(epoch))\n\n    #### Train ####\n    loss_mean = 0\n    kl_div_mean = 0\n    reconst_loss_mean = 0\n    nb_pass = 0\n    model.train()\n\n    for i, x in tqdm(enumerate(data_loader), total=len(dataset)//batch_size):\n        if arguments['--inputrep'] == \"notetuple\":\n            x[0] = x[0].to(device=device)  # type: ignore\n            x[1] = x[1].to(device=device)  # type: ignore\n        else:\n            x = x.to(device=device)  # type: ignore\n        # training pass\n        loss, kl_div, reconst_loss = model.batch_pass(x, loss_fn, optimizer,\n                                                      w_kl, dataset)\n        loss_mean += loss\n        kl_div_mean += kl_div\n        reconst_loss_mean += reconst_loss\n        nb_pass += 1\n\n    # Increase the kl weight\n    w_kl = increase_wkl(epoch, w_kl, arguments['--inputrep'])\n\n    #### Test ####\n    loss_mean_TEST = 0\n    kl_div_mean_TEST = 0\n    reconst_loss_mean_TEST = 0\n    nb_pass_TEST = 0\n    model.eval()\n\n    with torch.no_grad():\n        for i, x in tqdm(enumerate(test_loader), total=len(testset)//batch_size):\n            if arguments['--inputrep'] == \"notetuple\":\n                x[0] = x[0].to(device=device)  # type: ignore\n                x[1] = x[1].to(device=device)  # type: ignore\n            else:\n                x = x.to(device=device)  # type: ignore\n\n            # testing pass\n            loss, kl_div, reconst_loss = model.batch_pass(x, loss_fn, optimizer,\n                                                          w_kl, dataset, test=True)\n            loss_mean_TEST += loss\n            kl_div_mean_TEST += kl_div\n            reconst_loss_mean_TEST += reconst_loss\n            nb_pass_TEST += 1\n\n        #### Add to tensorboard ####\n        print(\"adding stuff to tensorboard\")\n        both_loss = {}\n        both_loss['train'] = loss_mean/nb_pass\n        both_loss['test'] = loss_mean_TEST/nb_pass_TEST\n        writer.add_scalar('data/loss_mean', loss_mean/nb_pass, epoch)\n        writer.add_scalar('data/kl_div_mean', kl_div_mean/nb_pass, epoch)\n        writer.add_scalar('data/reconst_loss_mean',\n                          reconst_loss_mean/nb_pass, epoch)\n        writer.add_scalar('data/loss_mean_TEST',\n                          loss_mean_TEST/nb_pass_TEST, epoch)\n        writer.add_scalar('data/kl_div_mean_TEST',\n                          kl_div_mean_TEST/nb_pass_TEST, epoch)\n        writer.add_scalar('data/reconst_loss_mean_TEST',\n                          reconst_loss_mean_TEST/nb_pass_TEST, epoch)\n        writer.add_scalars('data/Losses', both_loss, epoch)\n        \n\n    #### Save the model ####\n    if arguments['--save']:\n        if epoch > 0 and loss_mean_TEST < loss_test_min_reconst:\n            loss_test_min_reconst = loss_mean_TEST\n            os.makedirs(f\"{output_dr}/models/\", exist_ok=True)\n            torch.save(model.cpu().state_dict(),\n                       f\"{output_dr}/models/{arguments['--runname']}_epoch_{str(epoch+1)}.pth\")\n\n            model = model.to(device=device)  # type: ignore\n\n# End of the script, close the writer\nwriter.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#**Download the Outputs**\n\n!cd /kaggle/working\n!zip -r output.zip output/\n\nfrom IPython.display import FileLink \nFileLink(r'output.zip')","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-05-17T12:06:25.057911Z","iopub.status.idle":"2023-05-17T12:06:25.058401Z","shell.execute_reply.started":"2023-05-17T12:06:25.058187Z","shell.execute_reply":"2023-05-17T12:06:25.058205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cd /kaggle/working\n!rm output.zip","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-05-17T12:06:25.060325Z","iopub.status.idle":"2023-05-17T12:06:25.061001Z","shell.execute_reply.started":"2023-05-17T12:06:25.060794Z","shell.execute_reply":"2023-05-17T12:06:25.060811Z"},"trusted":true},"execution_count":null,"outputs":[]}]}